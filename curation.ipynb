{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/subreddit_minority_no_peer.yml\n",
      "为了使用邮件提醒功能，请设置环境变量MY_QQ_EMAIL（QQ邮箱地址）与MY_QQ_EMAIL_PWD（QQ邮箱授权码）\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:28:02 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "GPU ready...\n",
      "Smart using cuda:3\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"configs/subreddit_minority_no_peer.yml\"\n",
    "print(CONFIG_PATH)\n",
    "from utils import get_config, join_sets, load_model, print_log, save_model, load_model_dict\n",
    "config = get_config(CONFIG_PATH, \"_curation\", print_config = False)\n",
    "active_user_votes_thres = config[\"active_user_votes_thres\"]\n",
    "batch_size = config[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.5.0 of praw is outdated. Version 7.6.0 was released Tuesday May 10, 2022.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from superdebug import debug\n",
    "import torch\n",
    "from process_data import get_model_input\n",
    "from model import get_best_model\n",
    "from matplotlib import pyplot as plt\n",
    "from venn import venn, pseudovenn\n",
    "from utils import get_user_reps, get_bool_vec, record_existing_votes\n",
    "import time\n",
    "import re\n",
    "submission_sentiment_map = {}\n",
    "submission_class_map = {}\n",
    "submission_entity_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:28:07 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/process_data.py:363 get_model_input\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mLoading prepared data...\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:28:07 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:28:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['original_token_num'], at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/model.py:248 get_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 1.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36moriginal_token_num\u001b[0m\u001b[1;33m num val: 30522\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:28:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:28:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['latest_token_num'], at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/model.py:260 get_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 2.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mlatest_token_num\u001b[0m\u001b[1;33m num val: 219887\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:28:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:28:15 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:113 load_model\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mLoading best model...\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:28:15 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "target, original_feature_map, categorical_features, string_features, train_data, test_data, test_data_info, train_submission_upvote_df, num_all_users = get_model_input(config)\n",
    "extra_input = (categorical_features, string_features, target)\n",
    "model, token_embedding = get_best_model(config, categorical_features, string_features, original_feature_map)\n",
    "model.eval()\n",
    "all_users = list(range(max(max(train_data[\"USERNAME\"]), max(test_data[\"USERNAME\"])) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect submissions and active users in different subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddits_submissions(train_data:pd.DataFrame, test_data:pd.DataFrame, user_votes_thres = 0, max_test_submissions_per_subreddit = 9999999999999):\n",
    "    subreddit_votes_counter = Counter()\n",
    "    subreddit_user_vote_count = defaultdict(Counter)\n",
    "    subreddit_train_submissions = defaultdict(dict)\n",
    "    subreddit_test_submissions = defaultdict(dict)\n",
    "    all_submissions = {}\n",
    "    for i, row in train_data.iterrows():\n",
    "        subreddit_votes_counter[row[\"SUBREDDIT\"]] += 1\n",
    "        subreddit_user_vote_count[row[\"SUBREDDIT\"]][f'{row[\"USERNAME\"]}-{int(row[\"VOTE\"])}'] += 1\n",
    "        if row[\"SUBMISSION_ID\"] not in subreddit_train_submissions[row[\"SUBREDDIT\"]]:\n",
    "            subreddit_train_submissions[row[\"SUBREDDIT\"]][row[\"SUBMISSION_ID\"]] = row\n",
    "            all_submissions[row[\"SUBMISSION_ID\"]] = row\n",
    "    subreddit_active_users = defaultdict(set)\n",
    "    for subreddit in subreddit_user_vote_count:\n",
    "        users_vote_count = subreddit_user_vote_count[subreddit]\n",
    "        subreddit_active_users[subreddit] = {int(user_vote[:-2]) for user_vote in users_vote_count if users_vote_count[f\"{user_vote[:-2]}-1\"] + users_vote_count[f\"{user_vote[:-2]}-0\"] >= user_votes_thres}\n",
    "    test_data = test_data.sample(frac = 1)\n",
    "    for i, row in test_data.iterrows():\n",
    "        if row[\"SUBMISSION_ID\"] not in subreddit_test_submissions[row[\"SUBREDDIT\"]] and len(subreddit_test_submissions[row[\"SUBREDDIT\"]]) < max_test_submissions_per_subreddit:\n",
    "            subreddit_test_submissions[row[\"SUBREDDIT\"]][row[\"SUBMISSION_ID\"]] = row\n",
    "            all_submissions[row[\"SUBMISSION_ID\"]] = row\n",
    "    return subreddit_votes_counter, subreddit_active_users, subreddit_user_vote_count, subreddit_train_submissions, subreddit_test_submissions, all_submissions\n",
    "\n",
    "subreddit_votes_counter, subreddit_active_users, subreddit_user_vote_count, subreddit_train_submissions, subreddit_test_submissions, all_submissions = get_subreddits_submissions(train_data, test_data, user_votes_thres = active_user_votes_thres, max_test_submissions_per_subreddit=config[\"max_test_submissions_per_subreddit\"]) # subreddit_votes_counter, subreddit_users, subreddit_train_submissions are based on train_data, subreddit_test_submissions are based on test_data\n",
    "# print(sorted([(subreddit, len(subreddit_active_users[subreddit])) for subreddit in subreddit_active_users], key = lambda x: x[1], reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record down existing votes\n",
    "\n",
    "So that we can use them to substitute the predicted votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_votes, existing_user_votes, existing_user_updown_votes, existing_submission_votes, existing_user_subreddits = record_existing_votes(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict votes for all the users on all submissions\n",
    "\n",
    "Define required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from train import evaluate_model\n",
    "from model import get_tokenizer\n",
    "\n",
    "def convert_group_users_subreddit_submissions_data(group_users:Union[set,list], unique_submissions:dict):\n",
    "    group_users_submissions_data = []\n",
    "    if type(unique_submissions) == dict:\n",
    "        unique_submissions = pd.DataFrame(list(unique_submissions.values()))\n",
    "\n",
    "    for user in tqdm(group_users):\n",
    "        # for submission_id in unique_submissions:\n",
    "        #     submission:pd.DataFrame = unique_submissions[submission_id].copy(deep=True)\n",
    "        #     submission[\"USERNAME\"] = user\n",
    "        #     group_users_submissions_data.append(submission)\n",
    "        submissions = unique_submissions.copy(deep=True)\n",
    "        submissions[\"USERNAME\"] = [user] * len(submissions) # it doesn't matter whether the user itself is in UPVOTED_USERS / DOWNVOTED_USERS: we will substitute it with real votes\n",
    "        group_users_submissions_data.append(submissions)\n",
    "\n",
    "    group_users_submissions_data = pd.concat(group_users_submissions_data,axis=0)\n",
    "    return group_users_submissions_data\n",
    "def predict_group_users_submissions_votes(config, model, group_users_submissions_data, extra_input):\n",
    "    # predict unseen votes\n",
    "    return evaluate_model(config, model, data=group_users_submissions_data, weights = None, batch_size=config[\"batch_size\"], sample_voted_users=False, extra_input = extra_input, ret = \"prediction\") # ndarray size: (3423664, 1)\n",
    "pred_all_user_submission_vote_score_matrix = None\n",
    "\n",
    "# model.device = \"cuda:0\"\n",
    "def get_group_user_submission_vote_score_matrix(predicted_group_users_submissions_votes:np.ndarray, group_users, group_users_submissions_data:pd.DataFrame, existing_votes, existing_pred_user_submission_vote_score_matrix = None, upvote_confidence_thres = 0.5):\n",
    "    all_sub_ids = group_users_submissions_data[\"SUBMISSION_ID\"].unique()\n",
    "    if existing_pred_user_submission_vote_score_matrix is not None:\n",
    "        pred_user_submission_vote_score_matrix = existing_pred_user_submission_vote_score_matrix\n",
    "    else:\n",
    "        pred_user_submission_vote_score_matrix = pd.DataFrame(- np.ones([max(group_users) + 1, len(all_sub_ids)], dtype = float), columns=all_sub_ids)\n",
    "    pred_user_submission_vote_matrix = pd.DataFrame(- np.ones([max(group_users) + 1, len(all_sub_ids)], dtype = int), columns=all_sub_ids) # use ground truth vote if available, -1 for not in data\n",
    "    each_submission_votes = {}\n",
    "    each_user_confidence = defaultdict(list)\n",
    "    submission_ids = group_users_submissions_data[\"SUBMISSION_ID\"].to_numpy()\n",
    "    usernames = group_users_submissions_data[\"USERNAME\"].to_numpy()\n",
    "    for row_i in tqdm(range(len(group_users_submissions_data))):\n",
    "        submission_id = submission_ids[row_i]\n",
    "        username = usernames[row_i]\n",
    "        if existing_pred_user_submission_vote_score_matrix is not None:\n",
    "            vote_score = existing_pred_user_submission_vote_score_matrix.at[username, submission_id]\n",
    "            vote_score = float(vote_score)\n",
    "            assert vote_score != -1, f\"BUG: submission_id: {submission_id}, username: {username}, vote_score: {vote_score}\"\n",
    "        else:\n",
    "            vote_score = predicted_group_users_submissions_votes[row_i, 0]\n",
    "            pred_user_submission_vote_score_matrix.loc[username, submission_id] = vote_score\n",
    "        if f'{username}-{submission_id}' not in existing_votes:\n",
    "            vote = int(vote_score >= upvote_confidence_thres)\n",
    "        else: # use existing votes if available\n",
    "            vote = int(existing_votes[f'{username}-{submission_id}'] >= 0.5)\n",
    "            \n",
    "        if submission_id not in each_submission_votes:\n",
    "            each_submission_votes[submission_id] = [0, 0]\n",
    "        each_user_confidence[username].append(abs(vote_score - 0.5))\n",
    "        pred_user_submission_vote_matrix.at[username, submission_id] = vote\n",
    "        each_submission_votes[submission_id][vote] += 1\n",
    "    assert (pred_user_submission_vote_matrix.to_numpy() != -1).sum() > 0\n",
    "    \n",
    "    # analyze user confidence\n",
    "    for username in each_user_confidence:\n",
    "        each_user_confidence[username] = float(np.mean(each_user_confidence[username]))\n",
    "        \n",
    "    # calculate %upvotes for each submission\n",
    "    for submission_id in each_submission_votes:\n",
    "        each_submission_votes[submission_id].append(each_submission_votes[submission_id][1] / (each_submission_votes[submission_id][0] + each_submission_votes[submission_id][1])) # %upvotes\n",
    "    return each_submission_votes, each_user_confidence, pred_user_submission_vote_score_matrix, pred_user_submission_vote_matrix\n",
    "def get_group_users_preferred_submissions(each_submission_votes:dict, upvote_ratio_thres = 0.5):\n",
    "    # sort submissions using %upvotes\n",
    "    group_submissions_ranking = list(each_submission_votes.keys())\n",
    "    group_submissions_ranking.sort(reverse=True, key=lambda id: each_submission_votes[id][-1])\n",
    "    \n",
    "    # include submissions to preferred_submissions where %upvotes is higher than threshold\n",
    "    group_preferred_submissions = set()\n",
    "    for submission_id in group_submissions_ranking:\n",
    "        if each_submission_votes[submission_id][-1] >= upvote_ratio_thres:\n",
    "            group_preferred_submissions.add(submission_id)\n",
    "    \n",
    "    return group_preferred_submissions, group_submissions_ranking\n",
    "\n",
    "def get_group_users_real_vote(group_users:Union[set,list], unique_submissions:dict, existing_votes, metric = \"upvote_rate\"):\n",
    "    all_sub_ids = list(unique_submissions.keys())\n",
    "    debug(group_users)\n",
    "    pred_user_submission_vote_score_matrix = pd.DataFrame(- np.ones([max(group_users) + 1, len(all_sub_ids)], dtype = float), columns=all_sub_ids)\n",
    "    pred_user_submission_vote_matrix = pd.DataFrame(- np.ones([max(group_users) + 1, len(all_sub_ids)], dtype = int), columns=all_sub_ids)\n",
    "    each_submission_votes = {}\n",
    "    for submission_id in unique_submissions:\n",
    "        each_submission_votes[submission_id] = [0, 0]\n",
    "    for username in group_users:\n",
    "        for submission_id in unique_submissions:\n",
    "            if f'{username}-{submission_id}' in existing_votes:\n",
    "                vote = int(existing_votes[f'{username}-{submission_id}'] >= 0.5)\n",
    "                pred_user_submission_vote_matrix.at[username, submission_id] = vote\n",
    "                pred_user_submission_vote_score_matrix[username, submission_id] = vote\n",
    "                each_submission_votes[submission_id][vote] += 1\n",
    "                \n",
    "    each_user_confidence = {user: [0.5] for user in group_users}\n",
    "    \n",
    "    # calculate %upvotes for each submission\n",
    "    for submission_id in each_submission_votes:\n",
    "        if metric == \"upvote_rate\":\n",
    "            if each_submission_votes[submission_id][0] + each_submission_votes[submission_id][1] == 0:\n",
    "                metric_res = -1\n",
    "            else:\n",
    "                metric_res = each_submission_votes[submission_id][1] / (each_submission_votes[submission_id][0] + each_submission_votes[submission_id][1])\n",
    "        elif metric == \"#upvote-#downvote\":\n",
    "            metric_res = each_submission_votes[submission_id][1] - each_submission_votes[submission_id][0]\n",
    "        each_submission_votes[submission_id].append(metric_res)\n",
    "        \n",
    "    \n",
    "    return each_submission_votes, each_user_confidence, pred_user_submission_vote_score_matrix, pred_user_submission_vote_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to model input, then run model to make predictions, and obtain prediction score matrix and vote matrix. Note that we use actual votes to replace predicted votes when available.\n",
    "\n",
    "_!!! This process can be time consuming & need more than 200G memory for medium sized dataset, and is not necessary if we use user_embedding to cluster active users_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"user_grouping_method\"] == \"predict_all_submissions\":\n",
    "    # Convert data to model input\n",
    "    # TODO: not all users\n",
    "    all_users_submissions_data = convert_group_users_subreddit_submissions_data(all_users, all_submissions)\n",
    "\n",
    "    # run model to make predictions\n",
    "    model.to(model.device)\n",
    "    predicted_all_users_submissions_votes = predict_group_users_submissions_votes(config, model, all_users_submissions_data, extra_input)\n",
    "    debug(predicted_all_users_submissions_votes=predicted_all_users_submissions_votes)\n",
    "\n",
    "    import pickle\n",
    "    pickle.dump(predicted_all_users_submissions_votes, open(\"output/predicted_all_users_submissions_votes.pt\", \"wb\"))\n",
    "    # predicted_all_users_submissions_votes = pickle.load(open(\"output/predicted_all_users_submissions_votes.pt\", \"rb\"))\n",
    "\n",
    "    # Obtain prediction score matrix and vote matrix. We use actual votes to replace predicted votes when available\n",
    "    all_submission_votes, all_users_confidence, pred_all_user_submission_vote_score_matrix, pred_all_user_submission_vote_matrix = get_group_user_submission_vote_score_matrix(predicted_all_users_submissions_votes, all_users, all_users_submissions_data, existing_votes)\n",
    "    \n",
    "    # all_users_preferred_submissions, all_preferred_submissions_ranking = get_group_users_preferred_submissions(all_submission_votes, upvote_ratio_thres = config[\"upvote_ratio_thres\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Pearson correlation between users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"user_grouping_method\"] == \"predict_all_submissions\":\n",
    "    debug((pred_all_user_submission_vote_matrix==-1).any())\n",
    "    debug(pred_all_user_submission_vote_score_matrix=pred_all_user_submission_vote_score_matrix, pred_all_user_submission_vote_matrix=pred_all_user_submission_vote_matrix)\n",
    "    vote_score_pearson_corr = np.corrcoef(pred_all_user_submission_vote_score_matrix) # (697, 697)\n",
    "    debug(vote_score_pearson_corr=vote_score_pearson_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform curation on a subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a subreddit from the most popular subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_subreddit = None\n",
    "selected_subreddit = config[\"selected_subreddit\"]\n",
    "selected_subreddit = \"r/Feminism r/MensRights r/lesbian r/LesbianActually r/gay r/trans\" # r/politics_r/Conservative_r/Liberal_r/Republican_r/democrats_r/VoteBlue\n",
    "# selected_subreddit = \"r/politics r/Conservative r/Liberal r/Republican r/democrats r/VoteBlue r/Feminism r/MensRights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mSelected subreddit: r/Feminism\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mIn train data, subreddit r/Feminism have 162 active users (who votes >= 5 times), 4349 votes and 3091 unique submissions. In test data, subreddit r/Feminism have 500 unique submissions.\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mSelected subreddit: r/MensRights\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mIn train data, subreddit r/MensRights have 582 active users (who votes >= 5 times), 19698 votes and 11686 unique submissions. In test data, subreddit r/MensRights have 500 unique submissions.\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mSelected subreddit: r/lesbian\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mIn train data, subreddit r/lesbian have 0 active users (who votes >= 5 times), 0 votes and 0 unique submissions. In test data, subreddit r/lesbian have 0 unique submissions.\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mSelected subreddit: r/LesbianActually\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mIn train data, subreddit r/LesbianActually have 75 active users (who votes >= 5 times), 1960 votes and 1320 unique submissions. In test data, subreddit r/LesbianActually have 454 unique submissions.\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mSelected subreddit: r/gay\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mIn train data, subreddit r/gay have 155 active users (who votes >= 5 times), 3382 votes and 2459 unique submissions. In test data, subreddit r/gay have 500 unique submissions.\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mSelected subreddit: r/trans\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mIn train data, subreddit r/trans have 70 active users (who votes >= 5 times), 2086 votes and 1587 unique submissions. In test data, subreddit r/trans have 470 unique submissions.\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:126 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mIn train data, subreddit r/Feminism r/MensRights r/lesbian r/LesbianActually r/gay r/trans have 1000 active users (who votes >= 5 times), 31475 votes and 20143 unique submissions. In test data, subreddit r/Feminism r/MensRights r/lesbian r/LesbianActually r/gay r/trans have 2424 unique submissions.\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:35:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def get_selected_subreddit_info(config, selected_subreddit, subreddit_active_users, subreddit_votes_counter, subreddit_train_submissions, subreddit_test_submissions, original_feature_map, active_user_votes_thres):\n",
    "    if selected_subreddit is None:\n",
    "        common_subreddits_counts = subreddit_votes_counter.most_common(100)\n",
    "        prompt = []\n",
    "        for subreddit_id, vote_counts in common_subreddits_counts:\n",
    "            if \" \" in subreddit_id: continue\n",
    "            subreddit_name_str = (original_feature_map['SUBREDDIT'][subreddit_id] + ', ') if 'SUBREDDIT' in original_feature_map else ''\n",
    "            prompt.append(f\"{subreddit_id}: {subreddit_name_str}{vote_counts} votes;\")\n",
    "        prompt = \"\\n\".join(prompt)\n",
    "        selected_subreddit = input(f\"{prompt}\\nSelect subreddits: \")\n",
    "    multi_subreddits = selected_subreddit.split(\" \")\n",
    "    if len(multi_subreddits) == 1:\n",
    "        multi_subreddits = selected_subreddit.split(\"_\")\n",
    "    if \"interest\" in multi_subreddits:\n",
    "        multi_subreddits.remove(\"interest\")\n",
    "    for sub in multi_subreddits:\n",
    "        if 'SUBREDDIT' in original_feature_map:\n",
    "            sub = int(sub)\n",
    "            subreddit_name_str = (f\" ({original_feature_map['SUBREDDIT'][sub]})\")\n",
    "        else:\n",
    "            subreddit_name_str =  ''\n",
    "        print_log(config[\"log_path\"], f\"Selected subreddit: {sub}{subreddit_name_str}\")\n",
    "        print_log(config[\"log_path\"], f\"In train data, subreddit {sub} have {len(subreddit_active_users[sub])} active users (who votes >= {active_user_votes_thres} times), {subreddit_votes_counter[sub]} votes and {len(subreddit_train_submissions[sub])} unique submissions. In test data, subreddit {sub} have {len(subreddit_test_submissions[sub])} unique submissions.\") \n",
    "        \n",
    "        if len(multi_subreddits) > 1: # when we are using more than one subreddits\n",
    "            assert sub in subreddit_active_users\n",
    "            subreddit_active_users[selected_subreddit].update(subreddit_active_users[sub])\n",
    "            subreddit_votes_counter[selected_subreddit] += subreddit_votes_counter[sub]\n",
    "            subreddit_train_submissions[selected_subreddit].update(subreddit_train_submissions[sub])\n",
    "            subreddit_test_submissions[selected_subreddit].update(subreddit_test_submissions[sub])\n",
    "            \n",
    "    selected_subreddit_active_users:set = subreddit_active_users[selected_subreddit]\n",
    "    print_log(config[\"log_path\"], f\"In train data, subreddit {selected_subreddit} have {len(selected_subreddit_active_users)} active users (who votes >= {active_user_votes_thres} times), {subreddit_votes_counter[selected_subreddit]} votes and {len(subreddit_train_submissions[selected_subreddit])} unique submissions. In test data, subreddit {selected_subreddit} have {len(subreddit_test_submissions[selected_subreddit])} unique submissions.\") \n",
    "    return selected_subreddit_active_users, subreddit_active_users, subreddit_votes_counter, subreddit_train_submissions, subreddit_test_submissions\n",
    "\n",
    "selected_subreddit_active_users, subreddit_active_users, subreddit_votes_counter, subreddit_train_submissions, subreddit_test_submissions = get_selected_subreddit_info(config, selected_subreddit, subreddit_active_users, subreddit_votes_counter, subreddit_train_submissions, subreddit_test_submissions, original_feature_map, active_user_votes_thres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze submissions content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze_post = config[\"analyze_post\"]\n",
    "analyze_post = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submissions_text(submission_ids, subreddit_test_submissions, selected_subreddit, pass_analyzed = False, submission_sentiment_map = None, submission_class_map = None, submission_entity_map = None):\n",
    "    _submission_ids = []\n",
    "    submissions_text = []\n",
    "    for submission_id in submission_ids:\n",
    "        # if \"SUBMISSION_ID\" in original_feature_map:\n",
    "        #     submission_id = original_feature_map[\"SUBMISSION_ID\"][submission_id]\n",
    "        # submissions_text.append(submission_text_map[submission_id])\n",
    "        if pass_analyzed:\n",
    "            if submission_id in submission_sentiment_map and submission_id in submission_class_map and submission_id in submission_entity_map:\n",
    "                continue\n",
    "        _submission_ids.append(submission_id)\n",
    "        submissions_text.append(subreddit_test_submissions[selected_subreddit][submission_id][\"SUBMISSION_TEXT\"])\n",
    "    if pass_analyzed:\n",
    "        return _submission_ids, submissions_text\n",
    "    else:\n",
    "        return submissions_text\n",
    "\n",
    "if analyze_post and config[\"submission_source\"] == \"test_data\":\n",
    "    import os\n",
    "    import json\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"google-service-account-file.json\"\n",
    "    from google.cloud import language_v1\n",
    "    from sqlalchemy import create_engine, Column, Integer, String\n",
    "    from sqlalchemy.orm import sessionmaker\n",
    "    from sqlalchemy.ext.declarative import declarative_base\n",
    "    import grpc, google\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "    \n",
    "    submission_analysis_path = \"data/reddit/submission_analysis.db\"\n",
    "    engine = create_engine(f\"sqlite:///{submission_analysis_path}\", connect_args={'timeout': 10})\n",
    "    DBSession = sessionmaker(bind=engine)\n",
    "    session = DBSession()\n",
    "\n",
    "    Base = declarative_base()\n",
    "    \n",
    "    class Analysis(Base):\n",
    "        __tablename__ = 'analysis'\n",
    "        id = Column(String, primary_key=True, autoincrement=True)\n",
    "        sentiment_score = Column(Integer)\n",
    "        content_classes = Column(String)\n",
    "        entities = Column(String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd960dadfa8c45e9b9e1bb4bcfbd8e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa29d4dec9204a00aed2077f499211bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def post_analysis(content, language_v1, client, google):\n",
    "    document = language_v1.Document(content=content.lower(), type_=language_v1.Document.Type.PLAIN_TEXT)\n",
    "    try:\n",
    "        sentiment_score = client.analyze_sentiment(document=document).document_sentiment.score\n",
    "    except google.api_core.exceptions.InvalidArgument: #grpc.RpcError:\n",
    "        sentiment_score = None\n",
    "    except google.api_core.exceptions.TooManyRequests:\n",
    "        sentiment_score = None\n",
    "    except google.api_core.exceptions.ResourceExhausted:\n",
    "        sentiment_score = None\n",
    "    except google.api_core.exceptions.InternalServerError:\n",
    "        sentiment_score = None\n",
    "    try:\n",
    "        content_classes = [cls.name for cls in client.classify_text(document=document).categories]\n",
    "    except google.api_core.exceptions.InvalidArgument: #grpc.RpcError:\n",
    "        content_classes = []\n",
    "    except google.api_core.exceptions.TooManyRequests:\n",
    "        content_classes = []\n",
    "    except google.api_core.exceptions.ResourceExhausted:\n",
    "        content_classes = []\n",
    "    except google.api_core.exceptions.InternalServerError:\n",
    "        content_classes = []\n",
    "    try:\n",
    "        entities = [ent.name for ent in client.analyze_entities(document=document).entities]\n",
    "    except google.api_core.exceptions.InvalidArgument: #grpc.RpcError:\n",
    "        entities = []\n",
    "    except google.api_core.exceptions.TooManyRequests:\n",
    "        entities = []\n",
    "    except google.api_core.exceptions.ResourceExhausted:\n",
    "        entities = []\n",
    "    except google.api_core.exceptions.InternalServerError:\n",
    "        entities = []\n",
    "        \n",
    "    return (sentiment_score, content_classes, entities)\n",
    "\n",
    "def post_analysis_batch(id_content_map:dict, session, Analysis, submission_sentiment_map, submission_class_map, submission_entity_map, language_v1, client, google):\n",
    "    ids = list(id_content_map.keys())\n",
    "    existing_items = []\n",
    "    for i in tqdm(range(len(ids) // 1000 + 1)):\n",
    "        existing_items.extend(session.query(Analysis).filter(Analysis.id.in_(ids[1000*i:1000*(i+1)])).all())\n",
    "    for item in existing_items:\n",
    "        if item.sentiment_score == -9999 and item.content_classes == \"[]\" and item.entities == \"[]\":\n",
    "            session.query(Analysis).filter(Analysis.id == item.id).delete()\n",
    "            continue\n",
    "        submission_sentiment_map[item.id] = item.sentiment_score if item.sentiment_score != -9999 else None\n",
    "        submission_class_map[item.id] = json.loads(item.content_classes)\n",
    "        submission_entity_map[item.id] = json.loads(item.entities)\n",
    "        del id_content_map[item.id]\n",
    "    session.commit()\n",
    "    id_remain = list(id_content_map.keys())\n",
    "    for sub_i, id in enumerate(tqdm(id_remain)):\n",
    "        sentiment_score, content_classes, entities = post_analysis(id_content_map[id], language_v1, client, google)\n",
    "        submission_sentiment_map[id] = sentiment_score\n",
    "        submission_class_map[id] = content_classes\n",
    "        submission_entity_map[id] = entities\n",
    "        session.add(Analysis(id=id, sentiment_score = sentiment_score if sentiment_score is not None else -9999, content_classes=json.dumps(content_classes), entities=json.dumps(entities)))\n",
    "        session.commit()    \n",
    "\n",
    "if analyze_post and config[\"submission_source\"] == \"test_data\":\n",
    "    # submission_text_map = test_data[[\"SUBMISSION_ID\", \"SUBMISSION_TEXT\"]].drop_duplicates(\"SUBMISSION_ID\").set_index(\"SUBMISSION_ID\").to_dict()['SUBMISSION_TEXT']\n",
    "\n",
    "    subreddit_submissions_ids = list(subreddit_test_submissions[selected_subreddit].keys())\n",
    "    subreddit_submissions_ids, subreddit_submissions_text = get_submissions_text(subreddit_submissions_ids, subreddit_test_submissions, selected_subreddit, pass_analyzed = True, submission_sentiment_map = submission_sentiment_map, submission_class_map = submission_class_map, submission_entity_map = submission_entity_map)\n",
    "    # subreddit_submissions_ids, subreddit_submissions_text\n",
    "    post_analysis_batch({id: text for id, text in zip(subreddit_submissions_ids, subreddit_submissions_text)}, session, Analysis, submission_sentiment_map, submission_class_map, submission_entity_map, language_v1, client, google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtain representations for active users\n",
    "\n",
    "User representation will be used to cluster users into groups if `user_grouping_method` is `neural` or `vote`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:40:18 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['user_grouping_method'], at \u001b[0m\u001b[1;32m<ipython-input-14-09cd896f6cf2>:12 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 3.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36muser_grouping_method\u001b[0m\u001b[1;33m str len 45: topic_abortion|birth_gun random_user_as_group\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:40:18 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_grouping_method = config[\"user_grouping_method\"] # \n",
    "user_grouping_method = \"topic_abortion|birth_gun random_user_as_group\"\n",
    "# user_grouping_method = \"interest_r/Feminism_r/MensRights random_user_as_group\"\n",
    "# user_grouping_method = \"reliability\"\n",
    "# user_grouping_method = \"interest_r/Conservative_r/Liberal_r/Republican_r/democrats_r/VoteBlue random_user_as_group none\"\n",
    "# user_grouping_method = \"url_www.buzzfeednews.com&www.huffpost.com&www.breitbart.com_www.nytimes.com&www.theatlantic.com&www.theguardian.com\"\n",
    "# user_grouping_method = \"interest_r/Feminism_r/MensRights\" #TODO: change this\n",
    "# user_grouping_method = \"interest_r/Conservative_r/Liberal_r/Republican_r/democrats_r/VoteBlue\" #TODO: change this\n",
    "\n",
    "manual_user_groups = config[\"manual_user_groups\"]\n",
    "# manual_user_groups = {\"Conservative\": {66, 39, 10, 44, 16, 60}, \"Democratic\":{0, 65, 64, 37, 49, 52, 20, 22, 23, 26, 29}}\n",
    "debug(user_grouping_method=user_grouping_method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either use this one... (clustering using vote prediction score on submissions in this subreddit, make sure `pred_all_user_submission_vote_score_matrix` is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred_all_user_submission_vote_score_matrix is not None:\n",
    "    debug(pred_all_user_submission_vote_score_matrix=pred_all_user_submission_vote_score_matrix)\n",
    "    subreddit_submissions_bool_vec = get_bool_vec(subreddit_test_submissions[selected_subreddit].keys(), pred_all_user_submission_vote_score_matrix.shape[1])\n",
    "    selected_subreddit_active_users_reps, selected_subreddit_active_user_i_user_map = get_user_reps(selected_subreddit_active_users, all_user_embedding=pred_all_user_submission_vote_score_matrix[:, subreddit_submissions_bool_vec], train_data=train_data, selected_submissions = subreddit_train_submissions[selected_subreddit], method = user_grouping_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or this one... (cluster using user_embedding or sparse actual votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:40:19 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 2 vars: ['max_user', 'max_selected_subreddit_active_users'], at \u001b[0m\u001b[1;32m<ipython-input-16-02c58a0ba750>:2 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 4.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mmax_user\u001b[0m\u001b[1;33m num val: 183781\u001b[0m\n",
      "\u001b[1;33m1 / 5.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mmax_selected_subreddit_active_users\u001b[0m\u001b[1;33m num val: 183569\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:40:19 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:40:20 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['user_embedding'], at \u001b[0m\u001b[1;32m<ipython-input-16-02c58a0ba750>:8 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 6.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36muser_embedding\u001b[0m\u001b[1;33m torch.Size with val:  torch.Size([183782, 256])\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:40:20 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:40:20 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['selected_subreddit_active_users_reps'], at \u001b[0m\u001b[1;32m<ipython-input-16-02c58a0ba750>:11 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 7.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mselected_subreddit_active_users_reps\u001b[0m\u001b[1;33m None\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:40:20 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if pred_all_user_submission_vote_score_matrix is None:\n",
    "    debug(max_user = max(all_users), max_selected_subreddit_active_users = max([int(_) for _ in selected_subreddit_active_users]))\n",
    "    all_username_tokens = [f\"USERNAME_{user_i}\" for user_i in all_users]\n",
    "    all_username_token_ids = torch.tensor(model.tokenizer.convert_tokens_to_ids(all_username_tokens))\n",
    "    all_username_token_ids = all_username_token_ids.to(model.device); model = model.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        user_embedding = model.lm_encoder.embeddings.word_embeddings(all_username_token_ids)\n",
    "        debug(user_embedding=user_embedding.shape)\n",
    "    # debug(all_username_tokens=all_username_tokens, all_username_token_ids=all_username_token_ids, user_embedding=user_embedding)\n",
    "    selected_subreddit_active_users_reps, selected_subreddit_active_user_i_user_map = get_user_reps(selected_subreddit_active_users, all_user_embedding=user_embedding, train_data=train_data, selected_submissions = subreddit_train_submissions[selected_subreddit], user_grouping_method = user_grouping_method)\n",
    "    debug(selected_subreddit_active_users_reps=selected_subreddit_active_users_reps) # NOTE: selected_subreddit_active_users_reps is not None only if user_grouping_method == \"neural\" or \"votes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtain the users' political affiliations and the bias of media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              source  reliability   bias\n",
      "url                                                     \n",
      "abcnews.go.com               abcnews        46.64  -4.66\n",
      "aljazeera.com              aljazeera        45.54  -4.44\n",
      "alternet.org                alternet        23.28 -19.34\n",
      "americanthinker.com  americanthinker        18.41  25.36\n",
      "apnews.com                        ap        49.05  -2.03\n",
      "...                              ...          ...    ...\n",
      "washingtontimes.com  washingtontimes        31.86  14.89\n",
      "wonkette.com                wonkette        17.28 -26.75\n",
      "worldtruth.tv           worldtruthtv         7.41   8.48\n",
      "wnd.com                          wnd        22.33  20.57\n",
      "zerohedge.com              zerohedge        25.60  14.82\n",
      "\n",
      "[120 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_url_reliability_bias():\n",
    "    reliability_bias_df = pd.read_csv(\"tools/scrape_adfontes_media/data/ad_fontes_media_sources_ratings__2022_01_17.csv\")\n",
    "    source_url_df = pd.read_csv(\"data/political_bias/ad fontes media statistics 11.2020-12.2020.csv\")\n",
    "    source_url_df[\"source\"] = source_url_df[\"source\"].map(lambda x: x.replace(\"-\", \"\"))\n",
    "    reliability_bias_df[\"source\"] = reliability_bias_df[\"source\"].map(lambda x: x.lower().replace(\" \", \"\"))\n",
    "    reliability_bias_df = source_url_df.merge(reliability_bias_df, on = \"source\", how = \"left\")\n",
    "    reliability_bias_df[\"reliability\"] = reliability_bias_df.apply(lambda row: row[\"reliability\"] if pd.notna(row[\"reliability\"]) else row[\"_reliability\"], axis = 1)\n",
    "    reliability_bias_df[\"bias\"] = reliability_bias_df.apply(lambda row: row[\"bias\"] if pd.notna(row[\"bias\"]) else row[\"_bias\"], axis = 1)\n",
    "    reliability_bias_df = reliability_bias_df[[\"source\", \"reliability\", \"bias\", \"url\"]]\n",
    "    reliability_bias_df.set_index(\"url\", inplace = True)\n",
    "    print(reliability_bias_df) # .iloc[:2]\n",
    "\n",
    "    media_url_re = \"(\"+\"|\".join(reliability_bias_df.index.to_list())+\")\"\n",
    "    media_url_re = media_url_re.replace(\".\", \"\\.\")\n",
    "    return reliability_bias_df, media_url_re\n",
    "\n",
    "def get_political_bias(submission_text, reliability_bias_df, media_url_re):\n",
    "    bias = []\n",
    "    urls = re.findall(media_url_re, submission_text)\n",
    "    for url in urls:\n",
    "        if url in reliability_bias_df.index:\n",
    "            bias.append(reliability_bias_df.loc[url][\"bias\"])\n",
    "    if bias:\n",
    "        return np.mean(bias)\n",
    "    else:\n",
    "        return np.nan\n",
    "def get_reliability(submission_text, reliability_bias_df, media_url_re):\n",
    "    bias = []\n",
    "    urls = re.findall(media_url_re, submission_text)\n",
    "    for url in urls:\n",
    "        if url in reliability_bias_df.index:\n",
    "            bias.append(reliability_bias_df.loc[url][\"reliability\"])\n",
    "    if bias:\n",
    "        return np.mean(bias)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "reliability_bias_df, media_url_re = get_url_reliability_bias()\n",
    "if \"SUBMISSION_BIAS\" not in train_data.columns:\n",
    "    train_data[\"SUBMISSION_BIAS\"] = train_data[\"SUBMISSION_URL\"].map(lambda x: get_political_bias(x, reliability_bias_df, media_url_re))\n",
    "    test_data[\"SUBMISSION_BIAS\"] = test_data[\"SUBMISSION_URL\"].map(lambda x: get_political_bias(x, reliability_bias_df, media_url_re))\n",
    "if \"SUBMISSION_RELIABILITY\" not in train_data.columns:\n",
    "    train_data[\"SUBMISSION_RELIABILITY\"] = train_data[\"SUBMISSION_URL\"].map(lambda x: get_reliability(x, reliability_bias_df, media_url_re))\n",
    "    test_data[\"SUBMISSION_RELIABILITY\"] = test_data[\"SUBMISSION_URL\"].map(lambda x: get_reliability(x, reliability_bias_df, media_url_re))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster active users into multiple groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_source_proportion = None\n",
    "from urllib.parse import urlparse\n",
    "if \"url\" in user_grouping_method:\n",
    "    url_groups = [_ for _ in user_grouping_method.split(\" \") if \"url\" in _][0].split(\"_\")\n",
    "    url_groups.remove(\"url\")\n",
    "    train_data[\"SUBMISSION_URL_DOMAIN\"] = train_data[\"SUBMISSION_URL\"].map(lambda x: urlparse(x).netloc)\n",
    "    test_data[\"SUBMISSION_URL_DOMAIN\"] = test_data[\"SUBMISSION_URL\"].map(lambda x: urlparse(x).netloc)\n",
    "    all_user_domains = Counter([f\"{username}_|_{domain}\" for username, domain in zip(train_data[\"USERNAME\"], train_data[\"SUBMISSION_URL_DOMAIN\"])])\n",
    "    user_source_proportion = defaultdict(dict)\n",
    "    for username in tqdm(selected_subreddit_active_users):\n",
    "        user_domains = {user_domain.split(\"_|_\")[-1]: count for user_domain, count in all_user_domains.items() if str(username) in user_domain}\n",
    "        for url_group in url_groups:\n",
    "            urls = url_group.split(\"&\")\n",
    "            user_source_proportion[username][url_group] = sum([user_domains[url] for url in urls if url in user_domains])/sum(user_domains.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/TableSense/anaconda3/envs/cr4/lib/python3.6/site-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:49:00 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['user_num_in_group'], at \u001b[0m\u001b[1;32m<ipython-input-19-2909143517dd>:168 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 8.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36muser_num_in_group\u001b[0m\u001b[1;33m dict {.} with 1 keys ['random_users']\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m    \u001b[0m\u001b[1;36mrandom_users\u001b[0m\u001b[1;33m num val: 50\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:49:00 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from reddit import get_user_info\n",
    "\n",
    "\n",
    "def get_user_groups(selected_subreddit, selected_users, selected_users_reps, selected_user_i_user_map:dict, user_grouping_method = \"neural\", existing_user_votes=None, manual_user_groups=None, train_data=None, original_feature_map=None, selected_submissions=None, model=None, subreddit_active_users=None, selected_subreddit_active_users=None, subreddit_user_vote_count = None, reliability_bias_df=None, media_url_re=None, user_source_proportion = None, extra_input=None):\n",
    "    group_centers = None\n",
    "    users_in_groups = defaultdict(set)\n",
    "    if \"upvote_downvote\" in user_grouping_method:\n",
    "        selected_submissions = list(selected_submissions.values())\n",
    "        while True:\n",
    "            some_submission_data = random.choice(selected_submissions)\n",
    "            use_submission = input(\"Use this submission?: \"+some_submission_data[\"SUBMISSION_TEXT\"])\n",
    "            if len(use_submission) > 0:\n",
    "                break\n",
    "        some_submission_data = pd.DataFrame([some_submission_data] * len(selected_users))\n",
    "        some_submission_data[\"USERNAME\"] = selected_users\n",
    "        predicted_some_submission_users_votes = predict_group_users_submissions_votes(config, model, some_submission_data, extra_input)\n",
    "        for row_i, user in enumerate(selected_users):\n",
    "            if predicted_some_submission_users_votes[row_i] >= 0.5:\n",
    "                users_in_groups[\"upvote\"].add(user)\n",
    "            else:\n",
    "                users_in_groups[\"downvote\"].add(user)\n",
    "        assert len(users_in_groups[\"upvote\"]) > 0 and len(users_in_groups[\"downvote\"]) > 0\n",
    "    \n",
    "    if \"interest\" in user_grouping_method:\n",
    "        interest_subreddits = [_ for _ in user_grouping_method.split(\" \") if \"interest\" in _][0].split(\"_\")\n",
    "        interest_subreddits.remove(\"interest\")\n",
    "        interest_subreddit_active_users = {}\n",
    "        debug(f\"Using the intersection of users active in {selected_subreddit} and users actively upvote (%upvote >= 0.7) in interest_subreddit as curators\")\n",
    "        for interest_subreddit in interest_subreddits:\n",
    "            if interest_subreddit not in subreddit_active_users: continue\n",
    "            interest_subreddit_active_users[interest_subreddit] = subreddit_active_users[interest_subreddit] & selected_subreddit_active_users\n",
    "            # debug(interest_subreddit, interest_subreddit_active_users[interest_subreddit])\n",
    "            interest_subreddit_active_users[interest_subreddit] = {user for user in interest_subreddit_active_users[interest_subreddit] if (subreddit_user_vote_count[interest_subreddit][f\"{user}-1\"]/(subreddit_user_vote_count[interest_subreddit][f\"{user}-1\"] + subreddit_user_vote_count[interest_subreddit][f\"{user}-0\"]) >= 0.7)} # NOTE: upvote users subreddit_user_vote_count\n",
    "        users_in_groups = {}\n",
    "        for interest_subreddit in interest_subreddits:\n",
    "            if interest_subreddit not in subreddit_active_users: continue\n",
    "            other_subreddit_active_users = set()\n",
    "            for other_subreddit in interest_subreddits:\n",
    "                if other_subreddit != interest_subreddit:\n",
    "                    other_subreddit_active_users.update(interest_subreddit_active_users[other_subreddit])\n",
    "            users_in_groups[interest_subreddit] = interest_subreddit_active_users[interest_subreddit] - other_subreddit_active_users\n",
    "    if \"topic\" in user_grouping_method:\n",
    "        topic_groups = [_ for _ in user_grouping_method.split(\" \") if \"topic\" in _][0].split(\"_\")\n",
    "        topic_groups.remove(\"topic\") \n",
    "        users_in_groups = defaultdict(set)\n",
    "        for topic_group in topic_groups:\n",
    "            topic_re = \"(\"+topic_group+\")\"\n",
    "            train_topic_data = train_data[train_data[\"SUBMISSION_TEXT\"].str.contains(topic_re)]\n",
    "            user_vote_topic_counter = Counter(f\"{user}-{vote}\" for user, vote in zip(train_topic_data[\"USERNAME\"], train_topic_data[\"VOTE\"]))\n",
    "            for user in set(train_data[\"USERNAME\"]):\n",
    "                if user_vote_topic_counter[f\"{user}-1\"] >= 3 and user_vote_topic_counter[f\"{user}-0\"] == 0:\n",
    "                    users_in_groups[topic_group].add(user)\n",
    "    if \"url\" in user_grouping_method:\n",
    "        users_in_groups = defaultdict(set)\n",
    "        for username in tqdm(selected_users):\n",
    "            for url_group in url_groups:\n",
    "                if user_source_proportion[username][url_group] >= 0.2:\n",
    "                    not_prefer_other = True\n",
    "                    for other_url_group in url_groups:\n",
    "                        if other_url_group != url_group and user_source_proportion[username][other_url_group] >= 0.03:\n",
    "                            not_prefer_other = False\n",
    "                            break\n",
    "                    if not_prefer_other:\n",
    "                        users_in_groups[url_group].add(username)                    \n",
    "            \n",
    "    if \"political_affiliation\" in user_grouping_method:\n",
    "        user_bias = defaultdict(list)\n",
    "        train_bias = train_data[\"SUBMISSION_BIAS\"].to_numpy()\n",
    "        all_usernames = train_data[\"USERNAME\"].to_list()\n",
    "        for row_i, username in enumerate(all_usernames):\n",
    "            if not np.isnan(train_bias[row_i]):\n",
    "                user_bias[username].append(train_bias[row_i])\n",
    "        for username in user_bias:\n",
    "            user_bias[username] = np.mean(user_bias[username])\n",
    "            if user_bias[username] < -15:\n",
    "                users_in_groups[\"left\"].add(username)\n",
    "            elif user_bias[username] > 15:\n",
    "                users_in_groups[\"right\"].add(username)\n",
    "                \n",
    "    if \"reliability\" in user_grouping_method:\n",
    "        user_bias = defaultdict(list)\n",
    "        train_reliability = train_data[\"SUBMISSION_RELIABILITY\"].to_numpy()\n",
    "        all_usernames = train_data[\"USERNAME\"].to_list()\n",
    "        for row_i, username in enumerate(all_usernames):\n",
    "            if not np.isnan(train_reliability[row_i]):\n",
    "                user_bias[username].append(train_reliability[row_i])\n",
    "        for username in user_bias:\n",
    "            if np.mean(user_bias[username]) < 25 and len(user_bias[username]) > 10:\n",
    "                users_in_groups[\"unreliable\"].add(username)\n",
    "            elif np.mean(user_bias[username]) > 45 and len(user_bias[username]) > 10:\n",
    "                users_in_groups[\"reliable\"].add(username)\n",
    "        \n",
    "    if \"manual\" in user_grouping_method:\n",
    "        if manual_user_groups is not None:\n",
    "            users_in_groups.update(manual_user_groups)\n",
    "        else:\n",
    "            user_preferences = defaultdict(dict)\n",
    "            usernames = train_data[\"USERNAME\"].to_list()\n",
    "            votes = train_data[\"VOTE\"].to_list()\n",
    "            submissions_text = train_data[\"SUBMISSION_TEXT\"].to_list()\n",
    "            for row_i, username in enumerate(usernames):\n",
    "                vote = votes[row_i]\n",
    "                submission_text = submissions_text[row_i]\n",
    "                if vote not in user_preferences[username]:\n",
    "                    user_preferences[username][vote] = []\n",
    "                user_preferences[username][vote].append(submission_text)\n",
    "            all_users = list(user_preferences.keys())\n",
    "            user_info_list = []\n",
    "            for _ in range(20):\n",
    "                possible_user = random.choice(all_users)\n",
    "                user_info, user_info_str = get_user_info(possible_user)\n",
    "                user_info[\"username\"] = possible_user\n",
    "                user_info[\"joined subreddits\"] = existing_user_subreddits[possible_user]\n",
    "                user_info_list.append(user_info)\n",
    "            user_info_df = pd.DataFrame.from_records(user_info_list).set_index(\"username\")\n",
    "            print(user_info_df)\n",
    "            users_in_groups[\"manual\"] = set([original_feature_map[\"USERNAME\"][username] for username in input(\"Enter usernames: \").split(\" \")])\n",
    "            \n",
    "    if \"neural\" in user_grouping_method or \"votes\" in user_grouping_method:\n",
    "        n_groups = int(len(selected_user_i_user_map) / 5) # TODO: change how many users in a group\n",
    "        debug(num_selected_users = len(selected_user_i_user_map), n_groups=n_groups) # n_groups: 118\n",
    "        debug(\"Begin grouping...\")\n",
    "        from sklearn.cluster import KMeans\n",
    "        grouping = KMeans(n_clusters = n_groups, random_state = 42, verbose = 0).fit(selected_users_reps)\n",
    "        group_centers = grouping.cluster_centers_\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        grouping = AgglomerativeClustering(linkage = \"complete\").fit(selected_users_reps)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import SpectralClustering\n",
    "        grouping = SpectralClustering(n_groups, random_state = 42, verbose = 0).fit(selected_users_reps)\n",
    "        \"\"\"\n",
    "        labels = grouping.labels_ # grouping.labels_: [584 350 948 ... 813 938 152]\n",
    "        \"\"\"\n",
    "        from sklearn.mixture import GaussianMixture\n",
    "        labels = GaussianMixture(n_groups, random_state = 42, verbose = 0).fit_predict(selected_users_reps)\n",
    "        \"\"\"\n",
    "        usernames_in_groups = defaultdict(set)\n",
    "        for user_i, group_x in enumerate(labels): \n",
    "            users_in_groups[group_x].add(selected_user_i_user_map[user_i])\n",
    "            usernames_in_groups[group_x].add(original_feature_map[\"USERNAME\"][selected_user_i_user_map[user_i]])\n",
    "        debug(group_user_num=str({group_x: len(users_in_groups[group_x]) for group_x in users_in_groups}))\n",
    "        debug(usernames_in_groups=str(usernames_in_groups))\n",
    "        \n",
    "    if  \"single_user_as_group\" in user_grouping_method or  \"all_user_as_group\" in user_grouping_method or \"random_user_as_group\" in user_grouping_method:\n",
    "        assert existing_user_votes is not None\n",
    "        all_users = list(selected_user_i_user_map.values())\n",
    "        all_users.sort(key=lambda x:existing_user_votes[x])\n",
    "        if \"single_user_as_group\" in user_grouping_method:\n",
    "            all_users = all_users[:10] + all_users[-10:]\n",
    "            for i, user in enumerate(all_users):\n",
    "                users_in_groups[i] = {user}\n",
    "            # users_in_groups = {i: {user} for i,user in selected_user_i_user_map.items()}\n",
    "        if \"all_user_as_group\" in user_grouping_method:\n",
    "            users_in_groups[\"all_users\"] = all_users\n",
    "        if \"random_user_as_group\" in user_grouping_method:\n",
    "            users_in_groups[\"random_users\"] = random.sample(all_users, 50)\n",
    "    \n",
    "    if \"none\" in user_grouping_method: # none: using broadcast\n",
    "        users_in_groups[\"none\"] = []\n",
    "            \n",
    "    return users_in_groups, group_centers\n",
    "\n",
    "\n",
    "if type(selected_subreddit_active_users_reps) == torch.tensor: selected_subreddit_active_users_reps = selected_subreddit_active_users_reps.cpu()\n",
    "users_in_groups, group_centers = get_user_groups(selected_subreddit, selected_subreddit_active_users, selected_subreddit_active_users_reps, selected_subreddit_active_user_i_user_map, user_grouping_method=user_grouping_method, existing_user_votes=existing_user_votes, manual_user_groups=manual_user_groups, train_data = train_data, original_feature_map=original_feature_map, selected_submissions = subreddit_train_submissions[selected_subreddit], model = model, subreddit_active_users=subreddit_active_users, selected_subreddit_active_users=selected_subreddit_active_users, subreddit_user_vote_count = subreddit_user_vote_count, reliability_bias_df=reliability_bias_df, media_url_re=media_url_re, user_source_proportion=user_source_proportion, extra_input=extra_input)\n",
    "debug(user_num_in_group = {x: len(y) for x, y in users_in_groups.items()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict preferred submissions of each group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select submissions to curate. Test submissions in this Subreddit or customized submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"submission_source\"] == \"test_data\":\n",
    "    submissions_before_curation:dict = subreddit_test_submissions[selected_subreddit]\n",
    "elif config[\"submission_source\"] == \"custom\":\n",
    "    custom_submission_id = \"custom_\" + str(random.randint(1000000000000000, 9999999999999999))\n",
    "    custom_username = input(\"Input author's username: \") # TODO:\n",
    "    custom_title = input(\"Input post title: \") # 'Employee of the year' 'This is so funny!!!'\n",
    "    custom_content = input(\"Input post content: \")\n",
    "    custom_submission = pd.Series({\n",
    "        'SUBMISSION_ID': custom_submission_id,\n",
    "        'SUBREDDIT': selected_subreddit,\n",
    "        'CREATED_TIME': re.sub(\"[0-9][0-9]:[0-9][0-9]:[0-9][0-9] \", \"\", time.ctime(time.time())),\n",
    "        'USERNAME': 9271, # TODO:\n",
    "        'VOTE': 1.0,\n",
    "        'TITLE': custom_title,\n",
    "        'AUTHOR': custom_username,\n",
    "        '#_COMMENTS': 0,\n",
    "        'NSFW': 'false',\n",
    "        'SCORE': 0,\n",
    "        'UPVOTED_%': 0.5,\n",
    "        'LINK': '',\n",
    "        'SUBMISSION_TEXT': (custom_title + \" [SEP] \" + custom_content) if custom_content != \"\" else custom_title,\n",
    "        'UPVOTED_USERS': [], # TODO:\n",
    "        'DOWNVOTED_USERS': []}\n",
    "    )\n",
    "    submissions_before_curation = {custom_submission_id: custom_submission}\n",
    "    print(submissions_before_curation)\n",
    "    \n",
    "pred_group_votes_info = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⭐️ Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "upvote_ratio_thres = 0.5 # TODO: # config[\"upvote_ratio_thres\"]\n",
    "upvote_confidence_thres = 0.5 # config[\"upvote_confidence_thres\"] # TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and show their relationship using venn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_groups_preferences(config, model, users_in_groups, unique_submissions:dict, subreddit_test_submissions, selected_subreddit, upvote_ratio_thres = 0.5, group_centers=None, user_grouping_method = \"rep\", existing_votes = None, existing_user_updown_votes = None, pred_group_votes_info = None, upvote_confidence_thres = 0.5, selected_subreddit_active_user_i_user_map=None, extra_input = None, display = True):\n",
    "    # users_in_groups = existing_user_votes.most_common(3)\n",
    "    groups_preferred_submissions = {}\n",
    "    groups_preferred_submissions_text = {}\n",
    "    # groups_submission_upvote_count_matrix = np.zeros([len(users_in_groups), len(unique_submissions)])\n",
    "    groups_submission_upvote_count_matrix = pd.DataFrame(np.zeros([len(users_in_groups), len(unique_submissions)]), index = list(users_in_groups.keys())) \n",
    "    unique_submissions_ids = list(unique_submissions.keys())\n",
    "    used_group_centers = []\n",
    "    if os.path.exists(config[\"preferred_submissions_venn_figure_dir\"]):\n",
    "        shutil.rmtree(config[\"preferred_submissions_venn_figure_dir\"])\n",
    "    os.makedirs(config[\"preferred_submissions_venn_figure_dir\"], exist_ok=True)\n",
    "    for group_x in users_in_groups:\n",
    "        if ((not (\"political_affiliation\" in user_grouping_method)) and (not (\"interest\" in user_grouping_method)) and (not (\"url\" in user_grouping_method)) and (not (\"upvote_downvote\" in user_grouping_method))) and (((not (\"single_user_as_group\" in user_grouping_method and len(users_in_groups[group_x]) == 1)) and (not (\"none\" in user_grouping_method and len(users_in_groups[group_x]) == 0)) and len(users_in_groups[group_x]) <= config[\"group_user_num_lower_thres\"]) or (\"all_user_as_group\" not in user_grouping_method and len(users_in_groups[group_x]) > config[\"group_user_num_upper_thres\"])): # keep middle sized centers\n",
    "            continue\n",
    "        if len(users_in_groups[group_x]) == 0:\n",
    "            continue\n",
    "        if group_centers is not None: # only keep not similar centers\n",
    "            group_x_center = group_centers[group_x]\n",
    "            similar_center = False\n",
    "            for center in used_group_centers:\n",
    "                if np.dot(group_x_center, center) > 0:\n",
    "                    similar_center = True\n",
    "                    break\n",
    "            if similar_center:\n",
    "                # continue\n",
    "                print_log(config[\"log_path\"], \"Have similar center with existing group\")\n",
    "            used_group_centers.append(group_x_center)\n",
    "\n",
    "        ################ predicting votes of some users and some submissions ##################\n",
    "        print_log(config[\"log_path\"], f\"Predicting group {group_x} with {len(users_in_groups[group_x])} users\") #  :{users_in_groups[group_x]}\n",
    "        \n",
    "        if len(users_in_groups[group_x]) > 0:\n",
    "            group_x_subreddit_submissions_data = convert_group_users_subreddit_submissions_data(users_in_groups[group_x], unique_submissions)\n",
    "            if group_x not in pred_group_votes_info or group_x == \"manual\":\n",
    "                predicted_group_x_submissions_votes = predict_group_users_submissions_votes(config, model, group_x_subreddit_submissions_data, extra_input)\n",
    "                group_x_submission_votes, group_x_confidence, pred_group_x_subreddit_submission_vote_score_matrix, pred_group_x_subreddit_submission_vote_matrix = get_group_user_submission_vote_score_matrix(predicted_group_x_submissions_votes, users_in_groups[group_x], group_x_subreddit_submissions_data, existing_votes, upvote_confidence_thres=upvote_confidence_thres)\n",
    "                \n",
    "                pred_group_votes_info[group_x] = pred_group_x_subreddit_submission_vote_score_matrix\n",
    "            else:\n",
    "                debug(\"Using existing pred_group_votes_info\")\n",
    "                pred_group_x_subreddit_submission_vote_score_matrix = pred_group_votes_info[group_x]\n",
    "                group_x_submission_votes, group_x_confidence, pred_group_x_subreddit_submission_vote_score_matrix, pred_group_x_subreddit_submission_vote_matrix = get_group_user_submission_vote_score_matrix(None, users_in_groups[group_x], group_x_subreddit_submissions_data, existing_votes, existing_pred_user_submission_vote_score_matrix=pred_group_x_subreddit_submission_vote_score_matrix, upvote_confidence_thres=upvote_confidence_thres)\n",
    "        elif len(users_in_groups[group_x]) == 0: # no user in this group, i.e., without curation, just count real votes\n",
    "            group_x_submission_votes, group_x_confidence, pred_group_x_subreddit_submission_vote_score_matrix, pred_group_x_subreddit_submission_vote_matrix = get_group_users_real_vote(list(selected_subreddit_active_user_i_user_map.values()), unique_submissions, existing_votes, metric = \"upvote_rate\")\n",
    "        \n",
    "        group_x_preferred_submissions, group_x_preferred_submissions_ranking = get_group_users_preferred_submissions(group_x_submission_votes, upvote_ratio_thres = upvote_ratio_thres)\n",
    "    \n",
    "        pred_group_x_subreddit_submission_vote_matrix_np = pred_group_x_subreddit_submission_vote_matrix[unique_submissions_ids].to_numpy()\n",
    "        pred_group_x_subreddit_submission_vote_matrix_np = pred_group_x_subreddit_submission_vote_matrix_np[pred_group_x_subreddit_submission_vote_matrix_np.sum(axis=1) >= 0]\n",
    "        groups_submission_upvote_count_matrix.loc[group_x] = pred_group_x_subreddit_submission_vote_matrix_np.sum(axis=0).astype(float)/len(pred_group_x_subreddit_submission_vote_matrix_np)\n",
    "        \n",
    "\n",
    "        group_x_preferred_ranked_submissions = group_x_preferred_submissions_ranking[:len(group_x_preferred_submissions)]\n",
    "        groups_preferred_submissions[group_x] = group_x_preferred_ranked_submissions # group_x_preferred_submissions\n",
    "        \n",
    "        # convert submission text content\n",
    "        group_x_preferred_ranked_submissions_text = get_submissions_text(group_x_preferred_ranked_submissions, subreddit_test_submissions, selected_subreddit)\n",
    "        groups_preferred_submissions_text[group_x] = group_x_preferred_ranked_submissions_text\n",
    "        \n",
    "        ################# Display submissions preferred by each group of users ######################\n",
    "        if display:\n",
    "            if \"single_user_as_group\" in user_grouping_method:\n",
    "                user_train_vote_prompt = f\"voted {existing_user_updown_votes[list(users_in_groups[group_x])[0]]} in training data, prediction confidence {list(group_x_confidence.values())[0]}, \"\n",
    "            else:\n",
    "                user_train_vote_prompt = \"\"\n",
    "                \n",
    "            \n",
    "                \n",
    "            print_log(config[\"log_path\"], f\"Users in group {group_x} {user_train_vote_prompt}prefers {len(group_x_preferred_submissions)}/{len(unique_submissions)} submissions (%upvotes ≥ {upvote_ratio_thres})\") #  (sorted using %upvotes): {group_x_preferred_ranked_submissions}, with text {group_x_preferred_ranked_submissions_text}\n",
    "\n",
    "            # draw venn diagram            \n",
    "            if len(groups_preferred_submissions) > 1 and len(groups_preferred_submissions) <=6 and sum([len(_) for _ in groups_preferred_submissions.values()]) > 0:\n",
    "                venn_input = {x: set(groups_preferred_submissions[x]) for x in groups_preferred_submissions}\n",
    "                ax = venn(venn_input) if len(groups_preferred_submissions) <=5 else pseudovenn(venn_input)\n",
    "                plt.show()\n",
    "                figure_path = f\"{config['preferred_submissions_venn_figure_dir']}/{len(groups_preferred_submissions)}_groups.png\"\n",
    "                ax.figure.savefig(figure_path)\n",
    "                debug(f\"Figure saved in {figure_path}\")\n",
    "    return groups_preferred_submissions, groups_preferred_submissions_text, groups_submission_upvote_count_matrix\n",
    "\n",
    "model = model.to(model.device); model.eval()\n",
    "groups_preferred_submissions, groups_preferred_submissions_text, groups_submission_upvote_count_matrix = predict_groups_preferences(config, model, users_in_groups, submissions_before_curation, subreddit_test_submissions, selected_subreddit, group_centers=group_centers, user_grouping_method=user_grouping_method, existing_votes=existing_votes, existing_user_updown_votes=existing_user_updown_votes, pred_group_votes_info = pred_group_votes_info, upvote_ratio_thres = upvote_ratio_thres, upvote_confidence_thres=upvote_confidence_thres, selected_subreddit_active_user_i_user_map=selected_subreddit_active_user_i_user_map, extra_input=extra_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show posting prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"submission_source\"] == \"custom\":\n",
    "    debug(groups_submission_upvote_count_matrix)\n",
    "    print(f\"You can post immediately in {[group_x for group_x in groups_preferred_submissions if len(groups_preferred_submissions[group_x]) > 0]}. You will need to wait for more votes to post in {[group_x for group_x in groups_preferred_submissions if len(groups_preferred_submissions[group_x]) == 0]} -- your post will stay in the background first. Currently, \", end = \"\")\n",
    "    for group_x in groups_preferred_submissions:\n",
    "        group_x_int = int(group_x.split(\" \")[-1])\n",
    "        print(f\"{100 * groups_submission_upvote_count_matrix[group_x_int, 0]}% of the curators in {group_x} are predicted to upvote on your post; \", end = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show preferred different posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:49:12 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m<ipython-input-24-b0be75983ba9>:9 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mgroups_preferred_submissions_text is empty\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:49:12 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "max_show_posts = 30\n",
    "if len(groups_preferred_submissions_text) > 0:\n",
    "    all_preferred_submissions_text = set.intersection(*[set(groups_preferred_submissions_text[group_x]) for group_x in groups_preferred_submissions_text])\n",
    "    # groups_preferred_submissions_text\n",
    "\n",
    "    for group_x in groups_preferred_submissions_text:\n",
    "        print(f\"Users in group {group_x} prefers {[_ for _ in groups_preferred_submissions_text[group_x] if _ in set(groups_preferred_submissions_text[group_x]) - all_preferred_submissions_text][:max_show_posts]}\") # \n",
    "else:\n",
    "    debug(\"groups_preferred_submissions_text is empty\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Pearson correlation of different groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson ranking items: ['random_users']\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:49:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['?'], at \u001b[0m\u001b[1;32m<ipython-input-25-0f8c7dde1ff3>:4 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 9.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36m?\u001b[0m\u001b[1;33m ndarray size: (0, 0) val: []\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-31 23:49:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Pearson ranking items:\", groups_submission_upvote_count_matrix.index.to_list())\n",
    "groups_submission_upvote_count_matrix_nonzero = groups_submission_upvote_count_matrix[groups_submission_upvote_count_matrix.sum(axis = 1) != 0]\n",
    "group_preference_pearson_corr = np.corrcoef(groups_submission_upvote_count_matrix_nonzero) # (697, 697)\n",
    "debug(group_preference_pearson_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import streamlit as st\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "def visualize_group_preferences(groups_preferred_submissions, test_data, user_grouping_method, submission_sentiment_map = None, submission_class_map=None, submission_entity_map=None, reliability_bias_df = None, media_url_re = None):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.update({\"deleted\", \"SEP\", \"sep\", \"http\", \"https\", \"com\", \"r\"})\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    submission_text_map = test_data[[\"SUBMISSION_ID\", \"SUBMISSION_TEXT\"]].set_index(\"SUBMISSION_ID\").to_dict()[\"SUBMISSION_TEXT\"]\n",
    "    submission_bias_map = test_data[[\"SUBMISSION_ID\", \"SUBMISSION_BIAS\"]].set_index(\"SUBMISSION_ID\").to_dict()[\"SUBMISSION_BIAS\"]\n",
    "    submission_reliability_map = test_data[[\"SUBMISSION_ID\", \"SUBMISSION_RELIABILITY\"]].set_index(\"SUBMISSION_ID\").to_dict()[\"SUBMISSION_RELIABILITY\"]\n",
    "    submission_subreddit_map = test_data[[\"SUBMISSION_ID\", \"SUBREDDIT\"]].set_index(\"SUBMISSION_ID\").to_dict()[\"SUBREDDIT\"]\n",
    "    submission_url_domain_map = None\n",
    "    if \"SUBMISSION_URL_DOMAIN\" in test_data.columns:\n",
    "        submission_url_domain_map = test_data[[\"SUBMISSION_ID\", \"SUBMISSION_URL_DOMAIN\"]].set_index(\"SUBMISSION_ID\").to_dict()[\"SUBMISSION_URL_DOMAIN\"]\n",
    "\n",
    "    all_classes = set()\n",
    "    groups_bias = {}\n",
    "    groups_reliability = {}\n",
    "    group_subreddit = {}\n",
    "    group_url_domain = {}\n",
    "    groups_class_counter_rate = {}\n",
    "    groups_sentiment = {}\n",
    "    groups_words = {}\n",
    "    for group_x in groups_preferred_submissions:\n",
    "        submission_sentiments = []\n",
    "        submission_bias = []\n",
    "        submission_reliability = []\n",
    "        submission_subreddit = []\n",
    "        submission_url_domain = []\n",
    "        submission_classes = []\n",
    "        submission_words = []\n",
    "        group_preferred_submissions = groups_preferred_submissions[group_x]\n",
    "        for submission_id in group_preferred_submissions:\n",
    "            if submission_sentiment_map is not None and submission_id in submission_sentiment_map and submission_sentiment_map[submission_id]:\n",
    "                submission_sentiments.append(submission_sentiment_map[submission_id])\n",
    "            if submission_class_map is not None and submission_id in submission_class_map:\n",
    "                submission_classes.extend(submission_class_map[submission_id])\n",
    "            if submission_entity_map is not None:\n",
    "                if submission_id in submission_entity_map:\n",
    "                    submission_words.extend(submission_entity_map[submission_id])\n",
    "                else:\n",
    "                    submission_words.extend([_ for _ in tokenizer.tokenize(submission_text_map[submission_id].lower()) if _ not in stop])\n",
    "            if not np.isnan(submission_bias_map[submission_id]):\n",
    "                submission_bias.append(submission_bias_map[submission_id])\n",
    "            if not np.isnan(submission_reliability_map[submission_id]):\n",
    "                submission_reliability.append(submission_reliability_map[submission_id])\n",
    "            submission_subreddit.append(submission_subreddit_map[submission_id])\n",
    "            if submission_url_domain_map is not None: submission_url_domain.append(submission_url_domain_map[submission_id])\n",
    "            \n",
    "    \n",
    "        groups_bias[group_x] = np.mean(submission_bias) if len(submission_bias) > 0 else 0\n",
    "        groups_reliability[group_x] = np.mean(submission_reliability) if len(submission_reliability) > 0 else 0\n",
    "        group_subreddit[group_x] = Counter(submission_subreddit)\n",
    "        group_url_domain[group_x] = Counter(submission_url_domain)\n",
    "        class_counter = Counter(submission_classes)\n",
    "        all_classes.update(class_counter.keys())\n",
    "        class_counter_rate = {k: v/len(group_preferred_submissions) for k, v in class_counter.items()}\n",
    "        groups_class_counter_rate[group_x] = class_counter_rate\n",
    "    \n",
    "        groups_sentiment[group_x] = np.mean(submission_sentiments) if len(submission_sentiments) > 0 else 0\n",
    "    \n",
    "        groups_words[group_x] = submission_words\n",
    "    all_classes = list(all_classes)\n",
    "    for group_x in groups_class_counter_rate:\n",
    "        class_counter_rate = groups_class_counter_rate[group_x]\n",
    "        preference_info_str = \"\"\n",
    "        preference_info_str += f\"{group_x}: \"\n",
    "        preference_info_str += f\"average bias {groups_bias[group_x]}, average reliability {groups_reliability[group_x]}, all subreddits {group_subreddit[group_x]}, \"\n",
    "        preference_info_str += f\"all url domains {group_url_domain[group_x]}\"\n",
    "        preference_info_str += f\"average sentiment {groups_sentiment[group_x]:.4f}, all classes {class_counter_rate}\"\n",
    "        submission_words_counter = Counter(groups_words[group_x])\n",
    "        if len(submission_words_counter) < 100:\n",
    "            preference_info_str += f\", all words {submission_words_counter}\\n\"\n",
    "        print(preference_info_str)\n",
    "        st.write(preference_info_str)\n",
    "    \n",
    "        # visualize wordcloud\n",
    "        if len(submission_words_counter) > 0:\n",
    "            wordcloud = WordCloud(stopwords=stop, background_color=\"white\").generate_from_frequencies(submission_words_counter)\n",
    "            ax = plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            st.pyplot(ax.figure)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    \n",
    "        # visualize topic distribution\n",
    "        if len(class_counter_rate) > 0:\n",
    "            class_counter_rate = {cls: [class_counter_rate.get(cls, 0)] for cls in all_classes}\n",
    "            class_counter_rate_df = pd.DataFrame.from_dict(class_counter_rate).transpose()\n",
    "            ax = sns.barplot(x=class_counter_rate_df.index, y=class_counter_rate_df[0])\n",
    "            ax.set(ylim=(0, 0.2))\n",
    "            plt.xticks(rotation=300)\n",
    "            st.pyplot(ax.figure)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "visualize_group_preferences(groups_preferred_submissions, test_data, user_grouping_method, submission_sentiment_map = submission_sentiment_map, submission_class_map=submission_class_map, submission_entity_map=submission_entity_map, reliability_bias_df=reliability_bias_df, media_url_re=media_url_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SUBMISSION_URL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9fcfe2eab56b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# np.mean(test_data[\"SUBMISSION_RELIABILITY\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# submissions_before_curation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msubreddit_test_submissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"r/politics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SUBMISSION_RELIABILITY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubreddit_test_submissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"r/politics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SUBMISSION_URL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_reliability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreliability_bias_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedia_url_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubreddit_test_submissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"r/politics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SUBMISSION_RELIABILITY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SUBMISSION_URL'"
     ]
    }
   ],
   "source": [
    "# np.mean(test_data[\"SUBMISSION_RELIABILITY\"])\n",
    "# submissions_before_curation\n",
    "subreddit_test_submissions[\"r/politics\"][\"SUBMISSION_RELIABILITY\"] = subreddit_test_submissions[\"r/politics\"][\"SUBMISSION_URL\"].map(lambda x: get_reliability(x, reliability_bias_df, media_url_re))\n",
    "np.mean(subreddit_test_submissions[\"r/politics\"][\"SUBMISSION_RELIABILITY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12 ('cr4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "740471d2bc5e6e0b41d12bdc2e64373746aa6a34800f381ff958ff5f02fa0c53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
