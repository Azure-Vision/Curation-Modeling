{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"configs/small_sample_sub_minority.yml\"\n",
    "from utils import get_config, join_sets, load_model, print_log, save_model, load_model_dict\n",
    "config = get_config(CONFIG_PATH, \"_curation\", print_config = False)\n",
    "active_user_votes_thres = config[\"active_user_votes_thres\"]\n",
    "batch_size = config[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from superdebug import debug\n",
    "import torch\n",
    "from process_data import get_model_input\n",
    "from model import get_best_model\n",
    "from matplotlib import pyplot as plt\n",
    "from venn import venn, pseudovenn\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:15:01 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/process_data.py:349 get_model_input\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mLoading prepared data...\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:15:01 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:15:04 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['original_token_num'], at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/model.py:248 get_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 1.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36moriginal_token_num\u001b[0m\u001b[1;33m num val: 30522\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:15:04 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:15:05 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['latest_token_num'], at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/model.py:260 get_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 2.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mlatest_token_num\u001b[0m\u001b[1;33m num val: 153833\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:15:05 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:15:06 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:110 load_model\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mLoading best model...\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:15:06 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "target, original_feature_map, categorical_features, string_features, train_data, test_data, test_data_info, train_submission_upvote_df, num_all_users = get_model_input(config)\n",
    "extra_input = (categorical_features, string_features, target)\n",
    "model, token_embedding = get_best_model(config, categorical_features, string_features, original_feature_map)\n",
    "model.eval()\n",
    "all_users = list(range(num_all_users + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect submissions and active users in different subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddits_submissions(train_data:pd.DataFrame, test_data:pd.DataFrame, user_votes_thres = 0):\n",
    "    subreddit_votes_counter = Counter()\n",
    "    subreddit_active_users = defaultdict(Counter)\n",
    "    subreddit_train_submissions = defaultdict(dict)\n",
    "    subreddit_test_submissions = defaultdict(dict)\n",
    "    all_submissions = dict()\n",
    "    for i, row in train_data.iterrows():\n",
    "        subreddit_votes_counter[row[\"SUBREDDIT\"]] += 1\n",
    "        subreddit_active_users[row[\"SUBREDDIT\"]][row[\"USERNAME\"]] += 1\n",
    "        if row[\"SUBMISSION_ID\"] not in subreddit_train_submissions[row[\"SUBREDDIT\"]]:\n",
    "            subreddit_train_submissions[row[\"SUBREDDIT\"]][row[\"SUBMISSION_ID\"]] = row\n",
    "            all_submissions[row[\"SUBMISSION_ID\"]] = row\n",
    "    for subreddit in subreddit_active_users:\n",
    "        users_vote_count = subreddit_active_users[subreddit]\n",
    "        subreddit_active_users[subreddit] = {user for user in users_vote_count if users_vote_count[user] >= user_votes_thres}\n",
    "    for i, row in test_data.iterrows():\n",
    "        if row[\"SUBMISSION_ID\"] not in subreddit_test_submissions[row[\"SUBREDDIT\"]]:\n",
    "            subreddit_test_submissions[row[\"SUBREDDIT\"]][row[\"SUBMISSION_ID\"]] = row\n",
    "            all_submissions[row[\"SUBMISSION_ID\"]] = row\n",
    "    return subreddit_votes_counter, subreddit_active_users, subreddit_train_submissions, subreddit_test_submissions, all_submissions\n",
    "\n",
    "subreddit_votes_counter, subreddit_active_users, subreddit_train_submissions, subreddit_test_submissions, all_submissions = get_subreddits_submissions(train_data, test_data, user_votes_thres = active_user_votes_thres) # subreddit_votes_counter, subreddit_users, subreddit_train_submissions are based on train_data, subreddit_test_submissions are based on test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record down existing votes\n",
    "\n",
    "So that we can use them to substitute the predicted votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_existing_votes(train_data:pd.DataFrame):\n",
    "    # collect existing votes\n",
    "    existing_votes = {}\n",
    "    existing_user_updown_votes = defaultdict(Counter)\n",
    "    existing_user_votes = Counter()\n",
    "    existing_submission_votes = defaultdict(Counter)\n",
    "    usernames = train_data[\"USERNAME\"].to_list()\n",
    "    sub_ids = train_data[\"SUBMISSION_ID\"].to_list()\n",
    "    votes = train_data[\"VOTE\"].to_list()\n",
    "    for row_i in range(len(train_data)):\n",
    "        existing_votes[f'{usernames[row_i]}-{sub_ids[row_i]}'] = votes[row_i]\n",
    "        existing_user_updown_votes[usernames[row_i]][votes[row_i]] += 1\n",
    "        existing_user_votes[usernames[row_i]] += 1\n",
    "        existing_submission_votes[sub_ids[row_i]][votes[row_i]] += 1\n",
    "    return existing_votes, existing_user_votes, existing_user_updown_votes, existing_submission_votes\n",
    "existing_votes, existing_user_votes, existing_user_updown_votes, existing_submission_votes = record_existing_votes(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict votes for all the users on all submissions\n",
    "\n",
    "Define required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from train import evaluate_model\n",
    "from model import get_tokenizer\n",
    "\n",
    "def convert_group_users_subreddit_submissions_data(group_users:Union[set,list], unique_submissions:dict):\n",
    "    group_users_submissions_data = []\n",
    "    if type(unique_submissions) == dict:\n",
    "        unique_submissions = pd.DataFrame(list(unique_submissions.values()))\n",
    "\n",
    "    for user in tqdm(group_users):\n",
    "        # for submission_id in unique_submissions:\n",
    "        #     submission:pd.DataFrame = unique_submissions[submission_id].copy(deep=True)\n",
    "        #     submission[\"USERNAME\"] = user\n",
    "        #     group_users_submissions_data.append(submission)\n",
    "        submissions = unique_submissions.copy(deep=True)\n",
    "        submissions[\"USERNAME\"] = [user] * len(submissions) # it doesn't matter whether the user itself is in UPVOTED_USERS / DOWNVOTED_USERS: we will substitute it with real votes\n",
    "        group_users_submissions_data.append(submissions)\n",
    "\n",
    "    group_users_submissions_data = pd.concat(group_users_submissions_data,axis=0)\n",
    "    return group_users_submissions_data\n",
    "def predict_group_users_submissions_votes(model, group_users_submissions_data, batch_size):\n",
    "    # predict unseen votes\n",
    "    return evaluate_model(config, model, data=group_users_submissions_data, weights = None, batch_size=config[\"batch_size\"], sample_voted_users=False, extra_input = extra_input, ret = \"prediction\") # ndarray size: (3423664, 1)\n",
    "pred_all_user_submission_vote_score_matrix = None\n",
    "\n",
    "# model.device = \"cuda:0\"\n",
    "def get_group_user_submission_vote_score_matrix(predicted_group_users_submissions_votes:np.ndarray, group_users, group_users_submissions_data:pd.DataFrame, existing_votes, existing_pred_user_submission_vote_score_matrix = None):\n",
    "    if existing_pred_user_submission_vote_score_matrix is not None:\n",
    "        pred_user_submission_vote_score_matrix = existing_pred_user_submission_vote_score_matrix\n",
    "    else:\n",
    "        all_sub_ids = group_users_submissions_data[\"SUBMISSION_ID\"].unique()\n",
    "        pred_user_submission_vote_score_matrix = pd.DataFrame(- np.ones([max(group_users) + 1, len(all_sub_ids)], dtype = float), columns=all_sub_ids)\n",
    "    pred_user_submission_vote_matrix = pd.DataFrame(- np.ones([max(group_users) + 1, len(all_sub_ids)], dtype = int), columns=all_sub_ids) # use ground truth vote if available, -1 for not in data\n",
    "    each_submission_votes = {}\n",
    "    each_user_confidence = defaultdict(list)\n",
    "    submission_ids = group_users_submissions_data[\"SUBMISSION_ID\"].to_numpy()\n",
    "    usernames = group_users_submissions_data[\"USERNAME\"].to_numpy()\n",
    "    for row_i in tqdm(range(len(group_users_submissions_data))):\n",
    "        submission_id = submission_ids[row_i]\n",
    "        username = usernames[row_i]\n",
    "        if existing_pred_user_submission_vote_score_matrix is not None:\n",
    "            vote_score = existing_pred_user_submission_vote_score_matrix[username, submission_id]\n",
    "        else:\n",
    "            vote_score = predicted_group_users_submissions_votes[row_i, 0]\n",
    "            pred_user_submission_vote_score_matrix[username, submission_id] = vote_score\n",
    "        if f'{username}-{submission_id}' not in existing_votes:\n",
    "            vote = int(vote_score >= 0.5)\n",
    "        else: # use existing votes if available\n",
    "            vote = int(existing_votes[f'{username}-{submission_id}'] >= 0.5)\n",
    "            \n",
    "        if submission_id not in each_submission_votes:\n",
    "            each_submission_votes[submission_id] = [0, 0]\n",
    "        each_user_confidence[username].append(abs(vote_score - 0.5))\n",
    "        pred_user_submission_vote_matrix.at[username, submission_id] = vote\n",
    "        each_submission_votes[submission_id][vote] += 1\n",
    "    assert (pred_user_submission_vote_matrix.to_numpy() != -1).sum() > 0\n",
    "    \n",
    "    # analyze user confidence\n",
    "    for username in each_user_confidence:\n",
    "        each_user_confidence[username] = float(np.mean(each_user_confidence[username]))\n",
    "        \n",
    "    # calculate %upvotes for each submission\n",
    "    for submission_id in each_submission_votes:\n",
    "        each_submission_votes[submission_id].append(each_submission_votes[submission_id][1] / (each_submission_votes[submission_id][0] + each_submission_votes[submission_id][1])) # %upvotes\n",
    "    return each_submission_votes, each_user_confidence, pred_user_submission_vote_score_matrix, pred_user_submission_vote_matrix\n",
    "def get_group_users_preferred_submissions(each_submission_votes:dict, thres = 0.9):\n",
    "    # sort submissions using %upvotes\n",
    "    group_submissions_ranking = list(each_submission_votes.keys())\n",
    "    group_submissions_ranking.sort(reverse=True, key=lambda id: each_submission_votes[id][-1])\n",
    "    \n",
    "    # include submissions to preferred_submissions where %upvotes is higher than threshold\n",
    "    group_preferred_submissions = set()\n",
    "    for submission_id in group_submissions_ranking:\n",
    "        if each_submission_votes[submission_id][-1] >= thres:\n",
    "            group_preferred_submissions.add(submission_id)\n",
    "    \n",
    "    return group_preferred_submissions, group_submissions_ranking\n",
    "\n",
    "def get_group_users_real_vote(group_users:Union[set,list], unique_submissions:dict, existing_votes, metric = \"upvote_rate\"):\n",
    "    all_sub_ids = list(unique_submissions.keys())\n",
    "    pred_user_submission_vote_score_matrix = pd.DataFrame(- np.ones([max(group_users) + 1, len(all_sub_ids)], dtype = float), columns=all_sub_ids)\n",
    "    pred_user_submission_vote_matrix = pd.DataFrame(- np.ones([max(group_users) + 1, len(all_sub_ids)], dtype = int), columns=all_sub_ids)\n",
    "    each_submission_votes = {}\n",
    "    for submission_id in unique_submissions:\n",
    "        each_submission_votes[submission_id] = [0, 0]\n",
    "    for username in group_users:\n",
    "        for submission_id in unique_submissions:\n",
    "            if f'{username}-{submission_id}' in existing_votes:\n",
    "                vote = int(existing_votes[f'{username}-{submission_id}'] >= 0.5)\n",
    "                pred_user_submission_vote_matrix.at[username, submission_id] = vote\n",
    "                pred_user_submission_vote_score_matrix[username, submission_id] = vote\n",
    "                each_submission_votes[submission_id][vote] += 1\n",
    "                \n",
    "    each_user_confidence = {user: [0.5] for user in group_users}\n",
    "    \n",
    "    # calculate %upvotes for each submission\n",
    "    for submission_id in each_submission_votes:\n",
    "        if metric == \"upvote_rate\":\n",
    "            if each_submission_votes[submission_id][0] + each_submission_votes[submission_id][1] == 0:\n",
    "                metric_res = -1\n",
    "            else:\n",
    "                metric_res = each_submission_votes[submission_id][1] / (each_submission_votes[submission_id][0] + each_submission_votes[submission_id][1])\n",
    "        elif metric == \"#upvote-#downvote\":\n",
    "            metric_res = each_submission_votes[submission_id][1] - each_submission_votes[submission_id][0]\n",
    "        each_submission_votes[submission_id].append(metric_res)\n",
    "        \n",
    "    \n",
    "    return each_submission_votes, each_user_confidence, pred_user_submission_vote_score_matrix, pred_user_submission_vote_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to model input, then run model to make predictions, and obtain prediction score matrix and vote matrix. Note that we use actual votes to replace predicted votes when available.\n",
    "\n",
    "_!!! This process can be time consuming & need more than 200G memory for medium sized dataset, and is not necessary if we use user_embedding to cluster active users_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"user_grouping_method\"] == \"predict_all_submissions\":\n",
    "    # Convert data to model input\n",
    "    # TODO: not all users\n",
    "    all_users_submissions_data = convert_group_users_subreddit_submissions_data(all_users, all_submissions)\n",
    "\n",
    "    # run model to make predictions\n",
    "    model.to(model.device)\n",
    "    predicted_all_users_submissions_votes = predict_group_users_submissions_votes(model, all_users_submissions_data, batch_size)\n",
    "    debug(predicted_all_users_submissions_votes=predicted_all_users_submissions_votes)\n",
    "\n",
    "    import pickle\n",
    "    pickle.dump(predicted_all_users_submissions_votes, open(\"output/predicted_all_users_submissions_votes.pt\", \"wb\"))\n",
    "    # predicted_all_users_submissions_votes = pickle.load(open(\"output/predicted_all_users_submissions_votes.pt\", \"rb\"))\n",
    "\n",
    "    # Obtain prediction score matrix and vote matrix. We use actual votes to replace predicted votes when available\n",
    "    all_submission_votes, all_users_confidence, pred_all_user_submission_vote_score_matrix, pred_all_user_submission_vote_matrix = get_group_user_submission_vote_score_matrix(predicted_all_users_submissions_votes, all_users, all_users_submissions_data, existing_votes)\n",
    "    \n",
    "    # all_users_preferred_submissions, all_preferred_submissions_ranking = get_group_users_preferred_submissions(all_submission_votes, thres = config[\"upvote_downvote_ratio_thres\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Pearson correlation between users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"user_grouping_method\"] == \"predict_all_submissions\":\n",
    "    debug((pred_all_user_submission_vote_matrix==-1).any())\n",
    "    debug(pred_all_user_submission_vote_score_matrix=pred_all_user_submission_vote_score_matrix, pred_all_user_submission_vote_matrix=pred_all_user_submission_vote_matrix)\n",
    "    vote_score_pearson_corr = np.corrcoef(pred_all_user_submission_vote_score_matrix) # (697, 697)\n",
    "    debug(vote_score_pearson_corr=vote_score_pearson_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform curation on a subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a subreddit from the most popular subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:03 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:123 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mSelected subreddit: r/funny\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:03 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:03 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:123 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mIn train data, subreddit r/funny have 47 active users (who votes >= 5 times), 4722 votes and 1539 unique submissions. In test data, subreddit r/funny have 604 unique submissions.\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:03 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "common_subreddits_counts = subreddit_votes_counter.most_common(20)\n",
    "prompt = []\n",
    "for subreddit_id, vote_counts in common_subreddits_counts:\n",
    "    subreddit_name_str = (original_feature_map['SUBREDDIT'][subreddit_id] + ', ') if 'SUBREDDIT' in original_feature_map else ''\n",
    "    prompt.append(f\"Subreddit {subreddit_id}: {subreddit_name_str}{vote_counts} votes\")\n",
    "prompt = \"\\n\".join(prompt)\n",
    "a_subreddit = input(f\"{prompt}\\nSelect a subreddit: \")\n",
    "if 'SUBREDDIT' in original_feature_map:\n",
    "    a_subreddit = int(a_subreddit)\n",
    "    subreddit_name_str = (f\" ({original_feature_map['SUBREDDIT'][a_subreddit]})\")\n",
    "else:\n",
    "    subreddit_name_str =  ''\n",
    "print_log(config[\"log_path\"], f\"Selected subreddit: {a_subreddit}{subreddit_name_str}\")\n",
    "a_subreddit_active_users:set = subreddit_active_users[a_subreddit]\n",
    "print_log(config[\"log_path\"], f\"In train data, subreddit {a_subreddit} have {len(a_subreddit_active_users)} active users (who votes >= {active_user_votes_thres} times), {subreddit_votes_counter[a_subreddit]} votes and {len(subreddit_train_submissions[a_subreddit])} unique submissions. In test data, subreddit {a_subreddit} have {len(subreddit_test_submissions[a_subreddit])} unique submissions.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:03 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['user_grouping_method'], at \u001b[0m\u001b[1;32m<ipython-input-10-c7ded1d0119f>:5 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 3.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36muser_grouping_method\u001b[0m\u001b[1;33m str len 5: votes\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:03 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_grouping_method = config[\"user_grouping_method\"]\n",
    "# user_grouping_method =  \"none\" # \"manual\" #TODO: change this\n",
    "manual_user_groups = config[\"manual_user_groups\"]\n",
    "# manual_user_groups = {\"Conservative\": {66, 39, 10, 44, 16, 60}, \"Democratic\":{0, 65, 64, 37, 49, 52, 20, 22, 23, 26, 29}}\n",
    "debug(user_grouping_method=user_grouping_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "def get_bool_vec(selected_ids, vec_size):\n",
    "    bool_vec = torch.zeros([vec_size], dtype = bool)\n",
    "    for user in selected_ids:\n",
    "        bool_vec[user] = True\n",
    "    return bool_vec\n",
    "\n",
    "def get_user_reps(selected_users, all_user_embedding, train_data:pd.DataFrame = None, selected_submissions = None, user_grouping_method = \"neural\"):\n",
    "    assert all_user_embedding is not None\n",
    "    selected_users_bool_vec = get_bool_vec(selected_users, all_user_embedding.shape[0])\n",
    "    # user_user_i_map = {}\n",
    "    selected_user_i_user_map = {}\n",
    "    user_i = 0\n",
    "    for user, in_subreddit in enumerate(selected_users_bool_vec):\n",
    "        if in_subreddit:\n",
    "            # user_user_i_map[user] = user_i\n",
    "            selected_user_i_user_map[user_i] = user\n",
    "            user_i += 1\n",
    "    # assert len(user_user_i_map) == len(user_i_user_map)\n",
    "    selected_users_reps = None\n",
    "    if \"neural\" in user_grouping_method:\n",
    "        selected_users_reps = all_user_embedding[selected_users_bool_vec, :]\n",
    "    elif \"votes\" in user_grouping_method:\n",
    "        assert train_data is not None and selected_submissions is not None\n",
    "        sub_sub_i_map = {sub: sub_i for sub_i, sub in enumerate(list(selected_submissions.keys()))}\n",
    "        users_reps = torch.zeros([all_user_embedding.shape[0], len(selected_submissions)])\n",
    "        for row_i, row in train_data.iterrows():\n",
    "            if row[\"USERNAME\"] in selected_users and row[\"SUBMISSION_ID\"] in selected_submissions:\n",
    "                vote = 1 if row[\"VOTE\"] == 1 else -1\n",
    "                users_reps[row[\"USERNAME\"], sub_sub_i_map[row[\"SUBMISSION_ID\"]]] = vote\n",
    "        selected_users_reps = users_reps[selected_users_bool_vec, :]\n",
    "        users_vote_sum = (selected_users_reps * selected_users_reps).sum(axis = -1, keepdim= True)\n",
    "        assert (users_vote_sum != 0).all()\n",
    "        # selected_users_reps = selected_users_reps / users_vote_sum # average votes on each submission\n",
    "        debug(selected_users_reps_before_PCA=selected_users_reps.shape)\n",
    "        pca_solver = sklearn.decomposition.PCA(n_components=0.95)\n",
    "        selected_users_reps = pca_solver.fit_transform(selected_users_reps)\n",
    "        debug(selected_users_reps_after_PCA = selected_users_reps.shape)\n",
    "\n",
    "    return selected_users_reps, selected_user_i_user_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtain representations for active users\n",
    "\n",
    "User representation will be used to cluster users into groups if `user_grouping_method` is \"`neural`\" or \"`vote`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either use this one... (clustering using vote prediction score on submissions in this subreddit, make sure `pred_all_user_submission_vote_score_matrix` is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred_all_user_submission_vote_score_matrix is not None:\n",
    "    debug(pred_all_user_submission_vote_score_matrix=pred_all_user_submission_vote_score_matrix)\n",
    "    subreddit_submissions_bool_vec = get_bool_vec(subreddit_test_submissions[a_subreddit].keys(), pred_all_user_submission_vote_score_matrix.shape[1])\n",
    "    a_subreddit_active_users_reps, a_subreddit_active_user_i_user_map = get_user_reps(a_subreddit_active_users, all_user_embedding=pred_all_user_submission_vote_score_matrix[:, subreddit_submissions_bool_vec], train_data=train_data, selected_submissions = subreddit_train_submissions[a_subreddit], method = user_grouping_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or this one... (cluster using user_embedding or sparse actual votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['selected_users_reps_before_PCA'], at \u001b[0m\u001b[1;32m<ipython-input-11-ebd605a469f0>:36 get_user_reps\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 4.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mselected_users_reps_before_PCA\u001b[0m\u001b[1;33m torch.Size with val:  torch.Size([47, 1539])\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['selected_users_reps_after_PCA'], at \u001b[0m\u001b[1;32m<ipython-input-11-ebd605a469f0>:39 get_user_reps\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 5.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mselected_users_reps_after_PCA\u001b[0m\u001b[1;33m tuple size: 2\u001b[0m\u001b[1;33m (..)\u001b[0m\n",
      "\u001b[1;33m    \u001b[0m\u001b[1;36mitem 0: \u001b[0m\u001b[1;33m num val: 47\u001b[0m\n",
      "\u001b[1;33m    \u001b[0m\u001b[1;36mitem 1: \u001b[0m\u001b[1;33m num val: 41\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:22 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['a_subreddit_active_users_reps'], at \u001b[0m\u001b[1;32m<ipython-input-13-6d87cb7ec640>:9 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 6.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36ma_subreddit_active_users_reps\u001b[0m\u001b[1;33m ndarray size: (47, 41) val: [[-0.25890883 -0.11345328  0.5527872  ... -0.17322755 -0.26432184\n",
      "  -0.59624279]\n",
      " [-0.22600384 -0.2881086  -0.15077428 ... -0.06213044 -0.07388871\n",
      "   0.24266361]\n",
      " [-0.19233876 -0.06853775 -0.01138292 ... -0.01035731 -0.02543764\n",
      "   0.05338689]\n",
      " ...\n",
      " [-0.2615407   0.01218455 -0.10054993 ... -1.10443874  0.53358253\n",
      "   0.04647484]\n",
      " [ 0.0198609  -0.97788323 -0.26807736 ...  0.32113076 -0.10214377\n",
      "   0.35736494]\n",
      " [-0.21304744 -0.2317038  -0.07850846 ... -0.00783067 -0.09919357\n",
      "   0.53045949]]\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if pred_all_user_submission_vote_score_matrix is None:\n",
    "    all_username_tokens = [f\"USERNAME_{user_i}\" for user_i in all_users]\n",
    "    all_username_token_ids = torch.tensor(model.tokenizer.convert_tokens_to_ids(all_username_tokens))\n",
    "    all_username_token_ids = all_username_token_ids.to(model.device); model = model.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        user_embedding = model.lm_encoder.embeddings.word_embeddings(all_username_token_ids)\n",
    "    # debug(all_username_tokens=all_username_tokens, all_username_token_ids=all_username_token_ids, user_embedding=user_embedding)\n",
    "    a_subreddit_active_users_reps, a_subreddit_active_user_i_user_map = get_user_reps(a_subreddit_active_users, all_user_embedding=user_embedding, train_data=train_data, selected_submissions = subreddit_train_submissions[a_subreddit], user_grouping_method = user_grouping_method)\n",
    "    debug(a_subreddit_active_users_reps=a_subreddit_active_users_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster active users into multiple groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 2 vars: ['num_selected_users', 'n_groups'], at \u001b[0m\u001b[1;32m<ipython-input-14-b734051e9167>:27 get_user_groups\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 7.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mnum_selected_users\u001b[0m\u001b[1;33m num val: 47\u001b[0m\n",
      "\u001b[1;33m1 / 8.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mn_groups\u001b[0m\u001b[1;33m num val: 9\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m<ipython-input-14-b734051e9167>:28 get_user_groups\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mBegin grouping...\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['group_user_num'], at \u001b[0m\u001b[1;32m<ipython-input-14-b734051e9167>:49 get_user_groups\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 9.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mgroup_user_num\u001b[0m\u001b[1;33m str len 55: {1: 28, 3: 1, 7: 1, 8: 8, 4: 1, 0: 1, 5: 5, 6: 1, 2: 1}\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['usernames_in_groups'], at \u001b[0m\u001b[1;32m<ipython-input-14-b734051e9167>:50 get_user_groups\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 10.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36musernames_in_groups\u001b[0m\u001b[1;33m str len 736: defaultdict(<class 'set'>, {1: {'LoftyGoat', 'relaxedguy', 'kalel1980', 'dilkoman', 'pmcall221', 'okay12', 'TigerWylde', 'ChokingTermite', 'blurbie', 'MRteddybear29', 'Tekuzo', 'aaaaa', 'vinsite', 'IAppreciatesReality', 'CallMeJase', 'jezebel_jessi', 'reggie991', 'milleribsen', 'yoduh4077', 'Cassieisnotclever', 'LifeIsProbablyMadeUp', 'Kahandran', 'Simba7', 'exhuma', 'Johnlovesyou', 'Babi_Gurrl', 'greifinn24', 'JoA_MoN'}, 3: {'LinkBrokeMyPots'}, 7: {'LoGMunKy'}, 8: {'lgtbyddrk', 'Seeminus', 'connectalllthedots', 'R ...  2: {'claudesoph'}})\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:16:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def get_user_groups(selected_users_reps, selected_user_i_user_map:dict, user_grouping_method = \"neural\", existing_user_votes=None, manual_user_groups=None, train_data=None):\n",
    "    group_centers = None\n",
    "    users_in_groups = defaultdict(set)\n",
    "    \n",
    "    if \"manual\" in user_grouping_method:\n",
    "        if manual_user_groups is not None:\n",
    "            users_in_groups.update(manual_user_groups)\n",
    "        else:\n",
    "            user_preferences = defaultdict(dict)\n",
    "            usernames = train_data[\"USERNAME\"].to_list()\n",
    "            votes = train_data[\"VOTE\"].to_list()\n",
    "            submissions_text = train_data[\"SUBMISSION_TEXT\"].to_list()\n",
    "            for row_i, username in enumerate(usernames):\n",
    "                vote = votes[row_i]\n",
    "                submission_text = submissions_text[row_i]\n",
    "                if vote not in user_preferences[username]:\n",
    "                    user_preferences[username][vote] = []\n",
    "                user_preferences[username][vote].append(submission_text)\n",
    "            all_users = list(user_preferences.keys())\n",
    "            for _ in range(20):\n",
    "                possible_user = random.choice(all_users)\n",
    "                if len(input(f\"Include user {possible_user}? \\n{user_preferences[possible_user]}\")) > 0:\n",
    "                    users_in_groups[len(users_in_groups)] = possible_user\n",
    "            \n",
    "    if \"neural\" in user_grouping_method or \"votes\" in user_grouping_method:\n",
    "        n_groups = int(len(selected_user_i_user_map) / 5) # TODO: change how many users in a group\n",
    "        debug(num_selected_users = len(selected_user_i_user_map), n_groups=n_groups) # n_groups: 118\n",
    "        debug(\"Begin grouping...\")\n",
    "        from sklearn.cluster import KMeans\n",
    "        grouping = KMeans(n_clusters = n_groups, random_state = 42, verbose = 0).fit(selected_users_reps)\n",
    "        group_centers = grouping.cluster_centers_\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        grouping = AgglomerativeClustering(linkage = \"complete\").fit(selected_users_reps)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import SpectralClustering\n",
    "        grouping = SpectralClustering(n_groups, random_state = 42, verbose = 0).fit(selected_users_reps)\n",
    "        \"\"\"\n",
    "        labels = grouping.labels_ # grouping.labels_: [584 350 948 ... 813 938 152]\n",
    "        \"\"\"\n",
    "        from sklearn.mixture import GaussianMixture\n",
    "        labels = GaussianMixture(n_groups, random_state = 42, verbose = 0).fit_predict(selected_users_reps)\n",
    "        \"\"\"\n",
    "        usernames_in_groups = defaultdict(set)\n",
    "        for user_i, group_x in enumerate(labels): \n",
    "            users_in_groups[group_x].add(selected_user_i_user_map[user_i])\n",
    "            usernames_in_groups[group_x].add(original_feature_map[\"USERNAME\"][selected_user_i_user_map[user_i]])\n",
    "        debug(group_user_num=str({group_x: len(users_in_groups[group_x]) for group_x in users_in_groups}))\n",
    "        debug(usernames_in_groups=str(usernames_in_groups))\n",
    "        \n",
    "    if  \"single_user_as_group\" in user_grouping_method or  \"all_user_as_group\" in user_grouping_method:\n",
    "        assert existing_user_votes is not None\n",
    "        all_users = list(selected_user_i_user_map.values())\n",
    "        all_users.sort(key=lambda x:existing_user_votes[x])\n",
    "        if  \"single_user_as_group\" in user_grouping_method:\n",
    "            all_users = all_users[:10] + all_users[-10:]\n",
    "            for i, user in enumerate(all_users):\n",
    "                users_in_groups[i] = {user}\n",
    "            # users_in_groups = {i: {user} for i,user in selected_user_i_user_map.items()}\n",
    "        if  \"all_user_as_group\" in user_grouping_method:\n",
    "            users_in_groups[len(users_in_groups)] = all_users\n",
    "    \n",
    "    if \"none\" in user_grouping_method:\n",
    "        users_in_groups[len(users_in_groups)] = []\n",
    "            \n",
    "    return users_in_groups, group_centers\n",
    "\n",
    "if type(a_subreddit_active_users_reps) == torch.tensor: a_subreddit_active_users_reps = a_subreddit_active_users_reps.cpu()\n",
    "users_in_groups, group_centers = get_user_groups(a_subreddit_active_users_reps, a_subreddit_active_user_i_user_map, user_grouping_method=user_grouping_method, existing_user_votes=existing_user_votes, manual_user_groups=manual_user_groups, train_data = train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict preferred submissions of each group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select submissions to curate. Test submissions in this Subreddit or customized submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_7913210495806781': SUBMISSION_ID      custom_7913210495806781\n",
      "SUBREDDIT                          r/funny\n",
      "CREATED_TIME               Mon Aug  8 2022\n",
      "USERNAME                              9271\n",
      "VOTE                                     1\n",
      "TITLE                                   hi\n",
      "AUTHOR                               49562\n",
      "#_COMMENTS                               0\n",
      "NSFW                                 false\n",
      "SCORE                                    0\n",
      "UPVOTED_%                              0.5\n",
      "LINK                                      \n",
      "SUBMISSION_TEXT                         hi\n",
      "UPVOTED_USERS                           []\n",
      "DOWNVOTED_USERS                         []\n",
      "dtype: object}\n"
     ]
    }
   ],
   "source": [
    "if config[\"submission_source\"] == \"test_data\":\n",
    "    submissions_before_curation:dict = subreddit_test_submissions[a_subreddit]\n",
    "elif config[\"submission_source\"] == \"custom\":\n",
    "    custom_submission_id = \"custom_\" + str(random.randint(1000000000000000, 9999999999999999))\n",
    "    custom_username = 49562 # TODO:\n",
    "    custom_title = input(\"Input post title: \") # 'Employee of the year' 'This is so funny!!!'\n",
    "    custom_content = input(\"Input post content: \")\n",
    "    custom_submission = pd.Series({\n",
    "        'SUBMISSION_ID': custom_submission_id,\n",
    "        'SUBREDDIT': a_subreddit,\n",
    "        'CREATED_TIME': re.sub(\"[0-9][0-9]:[0-9][0-9]:[0-9][0-9] \", \"\", time.ctime(time.time())),\n",
    "        'USERNAME': 9271, # TODO:\n",
    "        'VOTE': 1.0,\n",
    "        'TITLE': custom_title,\n",
    "        'AUTHOR': custom_username,\n",
    "        '#_COMMENTS': 0,\n",
    "        'NSFW': 'false',\n",
    "        'SCORE': 0,\n",
    "        'UPVOTED_%': 0.5,\n",
    "        'LINK': '',\n",
    "        'SUBMISSION_TEXT': (custom_title + \" [SEP] \" + custom_content) if custom_content != \"\" else custom_title,\n",
    "        'UPVOTED_USERS': [], # TODO:\n",
    "        'DOWNVOTED_USERS': []}\n",
    "    )\n",
    "    submissions_before_curation = {custom_submission_id: custom_submission}\n",
    "    print(submissions_before_curation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print original submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi']\n"
     ]
    }
   ],
   "source": [
    "# submission_text_map = test_data[[\"SUBMISSION_ID\", \"SUBMISSION_TEXT\"]].drop_duplicates(\"SUBMISSION_ID\").set_index(\"SUBMISSION_ID\").to_dict()['SUBMISSION_TEXT']\n",
    "\n",
    "def get_submissions_text(submission_ids):\n",
    "    submissions_text = []\n",
    "    for submission_id in submission_ids:\n",
    "        # if \"SUBMISSION_ID\" in original_feature_map:\n",
    "        #     submission_id = original_feature_map[\"SUBMISSION_ID\"][submission_id]\n",
    "        # submissions_text.append(submission_text_map[submission_id])\n",
    "        submissions_text.append(submissions_before_curation[submission_id][\"SUBMISSION_TEXT\"])\n",
    "        \n",
    "    return submissions_text\n",
    "\n",
    "print(get_submissions_text(submissions_before_curation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_group_votes_info = {}\n",
    "batch_size = 1024\n",
    "# thres = config[\"upvote_downvote_ratio_thres\"]\n",
    "thres = 0.5 # TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and show their relationship using venn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:123 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mPredicting group 1 with users {26240, 7811, 27395, 31107, 14601, 7563, 15381, 17190, 8617, 8757, 25272, 26812, 30407, 2507, 7509, 23254, 2904, 20313, 1501, 21985, 6629, 9067, 18539, 21103, 2676, 23669, 27384, 13436}\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:23 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78601b5a9c8549c8b2035fce5bd3d4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['sample_voted_users'], at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/dynamic_data.py:15 __init__\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 27.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36msample_voted_users\u001b[0m\u001b[1;33m bool: False\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.25it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a31766b04d94e329ee7868336fc409f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:123 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mUsers in group 1 prefers 0/1 submissions (%upvotes ≥ 0.5) (sorted using %upvotes): [], with text []\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:123 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mPredicting group 8 with users {13088, 29313, 20552, 22410, 14290, 19539, 12155, 24350}\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83db78e8922c4300b1c5958b213087d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['sample_voted_users'], at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/dynamic_data.py:15 __init__\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 28.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36msample_voted_users\u001b[0m\u001b[1;33m bool: False\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 107.52it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9dce3c3a96421ba0ce3de915cf77c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:123 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mUsers in group 8 prefers 0/1 submissions (%upvotes ≥ 0.5) (sorted using %upvotes): [], with text []\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:123 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mPredicting group 5 with users {27461, 25042, 15029, 21211, 27548}\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd169e7501904fc1aceeda980978a8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['sample_voted_users'], at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/dynamic_data.py:15 __init__\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 29.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36msample_voted_users\u001b[0m\u001b[1;33m bool: False\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 111.28it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56100978de5940a6aa454bcb96056558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:123 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mUsers in group 5 prefers 0/1 submissions (%upvotes ≥ 0.5) (sorted using %upvotes): [], with text []\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['?'], at \u001b[0m\u001b[1;32m<ipython-input-47-651a2459889b>:77 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 30.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36m?\u001b[0m\u001b[1;33m ndarray size: (9, 1) val: [[0.  ]\n",
      " [0.25]\n",
      " [0.  ]\n",
      " [0.  ]\n",
      " [0.  ]\n",
      " [0.2 ]\n",
      " [0.  ]\n",
      " [0.  ]\n",
      " [0.25]]\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-08 07:22:24 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "You can post immediately in []. You will need to wait for more votes to post in ['Group 1', 'Group 8', 'Group 5'] -- your post will stay in the background first. Currently, 25.0% of the curators in Group 1 are predicted to upvote on your post; 25.0% of the curators in Group 8 are predicted to upvote on your post; 20.0% of the curators in Group 5 are predicted to upvote on your post; "
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_groups_preferences(users_in_groups, unique_submissions:dict, thres = 0.5, group_centers=None, user_grouping_method = \"rep\", existing_votes = None, existing_user_updown_votes = None, pred_group_votes_info = None):\n",
    "    # users_in_groups = existing_user_votes.most_common(3)\n",
    "    groups_preferred_submissions = {}\n",
    "    groups_preferred_submissions_text = {}\n",
    "    groups_submission_upvote_count_matrix = np.zeros([len(users_in_groups), len(unique_submissions)])\n",
    "    unique_submissions_ids = list(unique_submissions.keys())\n",
    "    used_group_centers = []\n",
    "    if os.path.exists(config[\"preferred_submissions_venn_figure_dir\"]):\n",
    "        shutil.rmtree(config[\"preferred_submissions_venn_figure_dir\"])\n",
    "    os.makedirs(config[\"preferred_submissions_venn_figure_dir\"], exist_ok=True)\n",
    "    for group_x in users_in_groups:\n",
    "        if ((not (\"single_user_as_group\" in user_grouping_method and len(users_in_groups[group_x]) == 1)) and (not (\"none\" in user_grouping_method and len(users_in_groups[group_x]) == 0)) and len(users_in_groups[group_x]) <= config[\"group_user_num_lower_thres\"]) or (\"all_user_as_group\" not in user_grouping_method and len(users_in_groups[group_x]) > config[\"group_user_num_upper_thres\"]): # keep middle sized centers\n",
    "            continue\n",
    "        if group_centers is not None: # only keep not similar centers\n",
    "            group_x_center = group_centers[group_x]\n",
    "            similar_center = False\n",
    "            for center in used_group_centers:\n",
    "                if np.dot(group_x_center, center) > 0:\n",
    "                    similar_center = True\n",
    "                    break\n",
    "            if similar_center:\n",
    "                # continue\n",
    "                print_log(config[\"log_path\"], \"Have similar center with existing group\")\n",
    "            used_group_centers.append(group_x_center)\n",
    "\n",
    "        ################ predicting votes of some users and some submissions ##################\n",
    "        print_log(config[\"log_path\"], f\"Predicting group {group_x} with users {users_in_groups[group_x]}\")\n",
    "        \n",
    "        if group_x not in pred_group_votes_info and len(users_in_groups[group_x]) > 0:\n",
    "            group_x_subreddit_submissions_data = convert_group_users_subreddit_submissions_data(users_in_groups[group_x], unique_submissions)\n",
    "            predicted_group_x_submissions_votes = predict_group_users_submissions_votes(model, group_x_subreddit_submissions_data, batch_size)\n",
    "            group_x_submission_votes, group_x_confidence, pred_group_x_subreddit_submission_vote_score_matrix, pred_group_x_subreddit_submission_vote_matrix = get_group_user_submission_vote_score_matrix(predicted_group_x_submissions_votes, users_in_groups[group_x], group_x_subreddit_submissions_data, existing_votes)\n",
    "            \n",
    "            pred_group_votes_info[group_x] = (group_x_submission_votes, group_x_confidence, pred_group_x_subreddit_submission_vote_score_matrix, pred_group_x_subreddit_submission_vote_matrix)\n",
    "        elif len(users_in_groups[group_x]) == 0: # no user in this group, i.e., without curation, just count real votes\n",
    "            group_x_submission_votes, group_x_confidence, pred_group_x_subreddit_submission_vote_score_matrix, pred_group_x_subreddit_submission_vote_matrix = get_group_users_real_vote(list(a_subreddit_active_user_i_user_map.values()), unique_submissions, existing_votes, metric = \"upvote_rate\")\n",
    "        else:\n",
    "            debug(\"Using existing pred_group_votes_info\")\n",
    "            group_x_submission_votes, group_x_confidence, pred_group_x_subreddit_submission_vote_score_matrix, pred_group_x_subreddit_submission_vote_matrix = pred_group_votes_info[group_x]\n",
    "        \n",
    "        \n",
    "        group_x_preferred_submissions, group_x_preferred_submissions_ranking = get_group_users_preferred_submissions(group_x_submission_votes, thres = thres)\n",
    "    \n",
    "        pred_group_x_subreddit_submission_vote_matrix_np = pred_group_x_subreddit_submission_vote_matrix[unique_submissions_ids].to_numpy()\n",
    "        pred_group_x_subreddit_submission_vote_matrix_np = pred_group_x_subreddit_submission_vote_matrix_np[pred_group_x_subreddit_submission_vote_matrix_np.sum(axis=1) >= 0]\n",
    "        groups_submission_upvote_count_matrix[group_x] = pred_group_x_subreddit_submission_vote_matrix_np.sum(axis=0).astype(float)/len(pred_group_x_subreddit_submission_vote_matrix_np)\n",
    "        \n",
    "\n",
    "        ################# Display submissions preferred by each group of users ######################\n",
    "\n",
    "        if \"single_user_as_group\" in user_grouping_method:\n",
    "            user_train_vote_prompt = f\"voted {existing_user_updown_votes[list(users_in_groups[group_x])[0]]} in training data, prediction confidence {list(group_x_confidence.values())[0]}, \"\n",
    "        else:\n",
    "            user_train_vote_prompt = \"\"\n",
    "            \n",
    "        group_x_preferred_ranked_submissions = group_x_preferred_submissions_ranking[:len(group_x_preferred_submissions)]\n",
    "        groups_preferred_submissions[f\"Group {group_x}\"] = group_x_preferred_submissions\n",
    "        \n",
    "        # convert submission text content\n",
    "        group_x_preferred_ranked_submissions_text = get_submissions_text(group_x_preferred_ranked_submissions)\n",
    "        groups_preferred_submissions_text[f\"Group {group_x}\"] = group_x_preferred_ranked_submissions_text\n",
    "            \n",
    "        print_log(config[\"log_path\"], f\"Users in group {group_x} {user_train_vote_prompt}prefers {len(group_x_preferred_submissions)}/{len(unique_submissions)} submissions (%upvotes ≥ {thres}) (sorted using %upvotes): {group_x_preferred_ranked_submissions}, with text {group_x_preferred_ranked_submissions_text}\")\n",
    "\n",
    "        # draw venn diagram            \n",
    "        if len(groups_preferred_submissions) > 1 and len(groups_preferred_submissions) <=6 and sum([len(_) for _ in groups_preferred_submissions.values()]) > 0:\n",
    "            ax = venn(groups_preferred_submissions) if len(groups_preferred_submissions) <=5 else pseudovenn(groups_preferred_submissions)\n",
    "            plt.show()\n",
    "            figure_path = f\"{config['preferred_submissions_venn_figure_dir']}/{len(groups_preferred_submissions)}_groups.png\"\n",
    "            ax.figure.savefig(figure_path)\n",
    "            debug(f\"Figure saved in {figure_path}\")\n",
    "    return groups_preferred_submissions, groups_preferred_submissions_text, groups_submission_upvote_count_matrix\n",
    "\n",
    "model = model.to(model.device); model.eval()\n",
    "groups_preferred_submissions, groups_preferred_submissions_text, groups_submission_upvote_count_matrix = predict_groups_preferences(users_in_groups, submissions_before_curation, group_centers=group_centers, user_grouping_method=user_grouping_method, existing_votes=existing_votes, existing_user_updown_votes=existing_user_updown_votes, pred_group_votes_info = pred_group_votes_info, thres = thres)\n",
    "if config[\"submission_source\"] == \"custom\":\n",
    "    debug(groups_submission_upvote_count_matrix)\n",
    "    print(f\"You can post immediately in {[group_x for group_x in groups_preferred_submissions if len(groups_preferred_submissions[group_x]) > 0]}. You will need to wait for more votes to post in {[group_x for group_x in groups_preferred_submissions if len(groups_preferred_submissions[group_x]) == 0]} -- your post will stay in the background first. Currently, \", end = \"\")\n",
    "    for group_x in groups_preferred_submissions:\n",
    "        group_x_int = int(group_x.split(\" \")[-1])\n",
    "        print(f\"{100 * groups_submission_upvote_count_matrix[group_x_int, 0]}% of the curators in {group_x} are predicted to upvote on your post; \", end = \"\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Pearson correlation of different groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "# print(groups_submission_upvote_count_matrix)\n",
    "groups_submission_upvote_count_matrix_nonzero = groups_submission_upvote_count_matrix[groups_submission_upvote_count_matrix.sum(axis = 1) != 0]\n",
    "group_preference_pearson_corr = np.corrcoef(groups_submission_upvote_count_matrix_nonzero) # (697, 697)\n",
    "print(group_preference_pearson_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "import multiprocessing\n",
    "client = language_v1.LanguageServiceClient()\n",
    "\n",
    "def post_analysis(content):\n",
    "    document = language_v1.Document(\n",
    "        content=content, type_=language_v1.Document.Type.PLAIN_TEXT\n",
    "    )\n",
    "    sentiment_score = client.analyze_sentiment(document=document).document_sentiment.score\n",
    "    content_classes = [cls.name for cls in client.classify_text(document=document).categories]\n",
    "    return (sentiment_score, content_classes)\n",
    "\n",
    "def post_analysis_batch(contents:list):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes=cores)\n",
    "    return pool.map(post_analysis, contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12 ('cr4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "740471d2bc5e6e0b41d12bdc2e64373746aa6a34800f381ff958ff5f02fa0c53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
