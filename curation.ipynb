{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为了使用邮件提醒功能，请设置环境变量MY_QQ_EMAIL（QQ邮箱地址）与MY_QQ_EMAIL_PWD（QQ邮箱授权码）\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:12:37 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "GPU ready...\n",
      "Smart using cuda:3\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = \"configs/small_sample_sub_minority.yml\"\n",
    "from utils import get_config, join_sets, load_model, print_log, save_model, load_model_dict\n",
    "config = get_config(CONFIG_PATH, \"_curation\", print_config = False)\n",
    "active_user_votes_thres = config[\"active_user_votes_thres\"]\n",
    "batch_size = config[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from superdebug import debug\n",
    "import torch\n",
    "from process_data import get_model_input\n",
    "from model import get_best_model\n",
    "from matplotlib import pyplot as plt\n",
    "from venn import venn, pseudovenn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:12:39 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/process_data.py:345 get_model_input\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mLoading prepared data...\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:12:39 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:12:43 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['original_token_num'], at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/model.py:248 get_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 1.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36moriginal_token_num\u001b[0m\u001b[1;33m num val: 30522\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:12:43 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:12:43 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['latest_token_num'], at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/model.py:260 get_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 2.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mlatest_token_num\u001b[0m\u001b[1;33m num val: 153833\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:12:43 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:12:44 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:110 load_model\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mLoading best model...\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:12:44 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "target, original_feature_map, categorical_features, string_features, train_data, test_data, test_data_info, train_submission_upvote_df, num_all_users = get_model_input(config)\n",
    "extra_input = (categorical_features, string_features, target)\n",
    "model, token_embedding = get_best_model(config, categorical_features, string_features, original_feature_map)\n",
    "model.eval()\n",
    "all_users = list(range(num_all_users + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect submissions and active users in different subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddits_submissions(train_data:pd.DataFrame, test_data:pd.DataFrame, user_votes_thres = 0):\n",
    "    subreddit_votes_counter = Counter()\n",
    "    subreddit_active_users = defaultdict(Counter)\n",
    "    subreddit_train_submissions = defaultdict(dict)\n",
    "    subreddit_test_submissions = defaultdict(dict)\n",
    "    all_submissions = dict()\n",
    "    for i, row in train_data.iterrows():\n",
    "        subreddit_votes_counter[row[\"SUBREDDIT\"]] += 1\n",
    "        subreddit_active_users[row[\"SUBREDDIT\"]][row[\"USERNAME\"]] += 1\n",
    "        if row[\"SUBMISSION_ID\"] not in subreddit_train_submissions[row[\"SUBREDDIT\"]]:\n",
    "            subreddit_train_submissions[row[\"SUBREDDIT\"]][row[\"SUBMISSION_ID\"]] = row\n",
    "            all_submissions[row[\"SUBMISSION_ID\"]] = row\n",
    "    for subreddit in subreddit_active_users:\n",
    "        users_vote_count = subreddit_active_users[subreddit]\n",
    "        subreddit_active_users[subreddit] = {user for user in users_vote_count if users_vote_count[user] >= user_votes_thres}\n",
    "    for i, row in test_data.iterrows():\n",
    "        if row[\"SUBMISSION_ID\"] not in subreddit_test_submissions[row[\"SUBREDDIT\"]]:\n",
    "            subreddit_test_submissions[row[\"SUBREDDIT\"]][row[\"SUBMISSION_ID\"]] = row\n",
    "            all_submissions[row[\"SUBMISSION_ID\"]] = row\n",
    "    return subreddit_votes_counter, subreddit_active_users, subreddit_train_submissions, subreddit_test_submissions, all_submissions\n",
    "\n",
    "subreddit_votes_counter, subreddit_active_users, subreddit_train_submissions, subreddit_test_submissions, all_submissions = get_subreddits_submissions(train_data, test_data, user_votes_thres = active_user_votes_thres) # subreddit_votes_counter, subreddit_users, subreddit_train_submissions are based on train_data, subreddit_test_submissions are based on test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record down existing votes\n",
    "\n",
    "So that we can use them to substitute the predicted votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_existing_votes(train_data:pd.DataFrame):\n",
    "    # collect existing votes\n",
    "    existing_votes = {}\n",
    "    existing_user_updown_votes = defaultdict(Counter)\n",
    "    existing_user_votes = Counter()\n",
    "    existing_submission_votes = defaultdict(Counter)\n",
    "    usernames = train_data[\"USERNAME\"].to_list()\n",
    "    sub_ids = train_data[\"SUBMISSION_ID\"].to_list()\n",
    "    votes = train_data[\"VOTE\"].to_list()\n",
    "    for row_i in range(len(train_data)):\n",
    "        existing_votes[f'{usernames[row_i]}-{sub_ids[row_i]}'] = votes[row_i]\n",
    "        existing_user_updown_votes[usernames[row_i]][votes[row_i]] += 1\n",
    "        existing_user_votes[usernames[row_i]] += 1\n",
    "        existing_submission_votes[sub_ids[row_i]][votes[row_i]] += 1\n",
    "    return existing_votes, existing_user_votes, existing_user_updown_votes, existing_submission_votes\n",
    "existing_votes, existing_user_votes, existing_user_updown_votes, existing_submission_votes = record_existing_votes(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict votes for all the users on all submissions\n",
    "\n",
    "Define required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "from train import evaluate_model\n",
    "from model import get_tokenizer\n",
    "\n",
    "def convert_group_users_subreddit_submissions_data(group_users:Union[set,list], unique_submissions:dict):\n",
    "    group_users_submissions_data = []\n",
    "    if type(unique_submissions) == dict:\n",
    "        unique_submissions = pd.DataFrame(list(unique_submissions.values()))\n",
    "\n",
    "    for user in tqdm(group_users):\n",
    "        # for submission_id in unique_submissions:\n",
    "        #     submission:pd.DataFrame = unique_submissions[submission_id].copy(deep=True)\n",
    "        #     submission[\"USERNAME\"] = user\n",
    "        #     group_users_submissions_data.append(submission)\n",
    "        submissions = unique_submissions.copy(deep=True)\n",
    "        submissions[\"USERNAME\"] = [user] * len(submissions)\n",
    "        group_users_submissions_data.append(submissions)\n",
    "\n",
    "    group_users_submissions_data = pd.concat(group_users_submissions_data,axis=0)\n",
    "    return group_users_submissions_data\n",
    "def predict_group_users_submissions_votes(model, group_users_submissions_data, batch_size):\n",
    "    # predict unseen votes\n",
    "    return evaluate_model(config, model, data=group_users_submissions_data, weights = None, batch_size=config[\"batch_size\"], sample_voted_users=False, extra_input = extra_input, ret = \"prediction\") # ndarray size: (3423664, 1)\n",
    "pred_all_user_submission_vote_score_matrix = None\n",
    "\n",
    "# model.device = \"cuda:0\"\n",
    "def get_group_users_preferred_submissions(model, predicted_group_users_submissions_votes:np.ndarray, group_users, group_users_submissions_data:pd.DataFrame, train_data:pd.DataFrame, existing_votes, thres = 0.9, existing_pred_submission_user_vote_score_matrix = None):\n",
    "    if existing_pred_submission_user_vote_score_matrix is not None:\n",
    "        pred_submission_user_vote_score_matrix = existing_pred_submission_user_vote_score_matrix\n",
    "    else:\n",
    "        all_sub_ids = group_users_submissions_data[\"SUBMISSION_ID\"].unique()\n",
    "        pred_submission_user_vote_score_matrix = pd.DataFrame(- np.ones([max(group_users) + 1, len(all_sub_ids)], dtype = float), columns=all_sub_ids)\n",
    "    pred_submission_user_vote_matrix = pd.DataFrame(- np.ones([max(group_users) + 1, len(all_sub_ids)], dtype = int), columns=all_sub_ids) # use ground truth vote if available, -1 for not in data\n",
    "    each_submission_votes = {}\n",
    "    each_user_confidence = defaultdict(list)\n",
    "    submission_ids = group_users_submissions_data[\"SUBMISSION_ID\"].to_numpy()\n",
    "    usernames = group_users_submissions_data[\"USERNAME\"].to_numpy()\n",
    "    for row_i in tqdm(range(len(group_users_submissions_data))):\n",
    "        submission_id = submission_ids[row_i]\n",
    "        username = usernames[row_i]\n",
    "        if existing_pred_submission_user_vote_score_matrix is not None:\n",
    "            vote_score = existing_pred_submission_user_vote_score_matrix[username, submission_id]\n",
    "        else:\n",
    "            vote_score = predicted_group_users_submissions_votes[row_i, 0]\n",
    "            pred_submission_user_vote_score_matrix[username, submission_id] = vote_score\n",
    "        if submission_id not in each_submission_votes:\n",
    "            each_submission_votes[submission_id] = [0, 0]\n",
    "        each_user_confidence[username].append(abs(vote_score - 0.5))\n",
    "        if f'{username}-{submission_id}' not in existing_votes:\n",
    "            vote = int(vote_score >= 0.5)\n",
    "        else: # use existing votes if available\n",
    "            vote = int(existing_votes[f'{username}-{submission_id}'] >= 0.5)\n",
    "        pred_submission_user_vote_matrix[username, submission_id] = vote\n",
    "        each_submission_votes[submission_id][vote] += 1\n",
    "\n",
    "    # analyze user confidence\n",
    "    for username in each_user_confidence:\n",
    "        each_user_confidence[username] = float(np.mean(each_user_confidence[username]))\n",
    "\n",
    "    # include submissions to preferred_submissions where %upvotes is higher than threshold\n",
    "    group_preferred_submissions = set()\n",
    "    for submission_id in each_submission_votes:\n",
    "        each_submission_votes[submission_id].append(each_submission_votes[submission_id][1] / (each_submission_votes[submission_id][0] + each_submission_votes[submission_id][1])) # %upvotes\n",
    "        if each_submission_votes[submission_id][-1] >= thres:\n",
    "            group_preferred_submissions.add(submission_id)\n",
    "\n",
    "    # sort submissions using %upvotes\n",
    "    group_submissions_ranking = list(each_submission_votes.keys())\n",
    "    group_submissions_ranking.sort(reverse=True, key=lambda id: each_submission_votes[id][-1])\n",
    "    return group_preferred_submissions, group_submissions_ranking, each_submission_votes, each_user_confidence, pred_submission_user_vote_score_matrix, pred_submission_user_vote_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to model input, then run model to make predictions, and obtain prediction score matrix and vote matrix. Note that we use actual votes to replace predicted votes when available.\n",
    "\n",
    "_!!! This process can be time consuming & need more than 200G memory for medium sized dataset, and is not necessary if we use user_embedding to cluster active users_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"user_grouping_method\"] == \"predict_all_submissions\":\n",
    "    # Convert data to model input\n",
    "    # TODO: not all users\n",
    "    all_users_submissions_data = convert_group_users_subreddit_submissions_data(all_users, all_submissions)\n",
    "\n",
    "    # run model to make predictions\n",
    "    model.to(model.device)\n",
    "    predicted_all_users_submissions_votes = predict_group_users_submissions_votes(model, all_users_submissions_data, batch_size)\n",
    "    debug(predicted_all_users_submissions_votes=predicted_all_users_submissions_votes)\n",
    "\n",
    "    import pickle\n",
    "    pickle.dump(predicted_all_users_submissions_votes, open(\"output/predicted_all_users_submissions_votes.pt\", \"wb\"))\n",
    "    # predicted_all_users_submissions_votes = pickle.load(open(\"output/predicted_all_users_submissions_votes.pt\", \"rb\"))\n",
    "\n",
    "    # Obtain prediction score matrix and vote matrix. We use actual votes to replace predicted votes when available\n",
    "    all_users_preferred_submissions, all_preferred_submissions_ranking, all_submission_votes, all_users_confidence, pred_all_user_submission_vote_score_matrix, pred_all_user_submission_vote_matrix = get_group_users_preferred_submissions(model, predicted_all_users_submissions_votes, all_users, all_users_submissions_data, train_data, existing_votes, thres = config[\"upvote_downvote_ratio_thres\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Pearson correlation between users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"user_grouping_method\"] == \"predict_all_submissions\":\n",
    "    debug((pred_all_user_submission_vote_matrix==-1).any())\n",
    "    debug(pred_all_user_submission_vote_score_matrix=pred_all_user_submission_vote_score_matrix, pred_all_user_submission_vote_matrix=pred_all_user_submission_vote_matrix)\n",
    "    vote_score_pearson_corr = np.corrcoef(pred_all_user_submission_vote_score_matrix) # (697, 697)\n",
    "    debug(vote_score_pearson_corr=vote_score_pearson_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform curation on a subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a subreddit from the most popular subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:18:14 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:123 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mSelected subreddit: r/funny\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:18:14 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:18:14 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:123 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mIn train data, subreddit r/funny have 47 active users (who votes >= 5 times), 4722 votes and 1539 unique submissions. In test data, subreddit r/funny have 604 unique submissions.\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:18:14 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "common_subreddits_counts = subreddit_votes_counter.most_common(20)\n",
    "prompt = []\n",
    "for subreddit_id, vote_counts in common_subreddits_counts:\n",
    "    subreddit_name_str = (original_feature_map['SUBREDDIT'][subreddit_id] + ', ') if 'SUBREDDIT' in original_feature_map else ''\n",
    "    prompt.append(f\"Subreddit {subreddit_id}: {subreddit_name_str}{vote_counts} votes\")\n",
    "prompt = \"\\n\".join(prompt)\n",
    "a_subreddit = input(f\"{prompt}\\nSelect a subreddit: \")\n",
    "if 'SUBREDDIT' in original_feature_map:\n",
    "    a_subreddit = int(a_subreddit)\n",
    "    subreddit_name_str = (f\" ({original_feature_map['SUBREDDIT'][a_subreddit]})\")\n",
    "else:\n",
    "    subreddit_name_str =  ''\n",
    "print_log(config[\"log_path\"], f\"Selected subreddit: {a_subreddit}{subreddit_name_str}\")\n",
    "a_subreddit_active_users:set = subreddit_active_users[a_subreddit]\n",
    "print_log(config[\"log_path\"], f\"In train data, subreddit {a_subreddit} have {len(a_subreddit_active_users)} active users (who votes >= {active_user_votes_thres} times), {subreddit_votes_counter[a_subreddit]} votes and {len(subreddit_train_submissions[a_subreddit])} unique submissions. In test data, subreddit {a_subreddit} have {len(subreddit_test_submissions[a_subreddit])} unique submissions.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:18:14 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['user_grouping_method'], at \u001b[0m\u001b[1;32m<ipython-input-30-27c6d0d6f711>:5 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 21.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36muser_grouping_method\u001b[0m\u001b[1;33m str len 6: neural\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:18:14 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_grouping_method = config[\"user_grouping_method\"]\n",
    "# user_grouping_method = \"neural\"\n",
    "manual_user_groups = config[\"manual_user_groups\"]\n",
    "# manual_user_groups = {\"Conservative\": {66, 39, 10, 44, 16, 60}, \"Democratic\":{0, 65, 64, 37, 49, 52, 20, 22, 23, 26, 29}}\n",
    "debug(user_grouping_method=user_grouping_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bool_vec(selected_ids, vec_size):\n",
    "    bool_vec = torch.zeros([vec_size], dtype = bool)\n",
    "    for user in selected_ids:\n",
    "        bool_vec[user] = True\n",
    "    return bool_vec\n",
    "\n",
    "def get_user_reps(selected_users, all_user_embedding, train_data:pd.DataFrame = None, selected_submissions = None, method = \"neural\"):\n",
    "    assert all_user_embedding is not None\n",
    "    selected_users_bool_vec = get_bool_vec(selected_users, all_user_embedding.shape[0])\n",
    "    # user_user_i_map = {}\n",
    "    selected_user_i_user_map = {}\n",
    "    user_i = 0\n",
    "    for user, in_subreddit in enumerate(selected_users_bool_vec):\n",
    "        if in_subreddit:\n",
    "            # user_user_i_map[user] = user_i\n",
    "            selected_user_i_user_map[user_i] = user\n",
    "            user_i += 1\n",
    "    # assert len(user_user_i_map) == len(user_i_user_map)\n",
    "    selected_users_reps = None\n",
    "    if method == \"neural\":\n",
    "        selected_users_reps = all_user_embedding[selected_users_bool_vec, :]\n",
    "    elif method == \"votes\":\n",
    "        assert train_data is not None and selected_submissions is not None\n",
    "        sub_sub_i_map = {sub: sub_i for sub_i, sub in enumerate(list(selected_submissions.keys()))}\n",
    "        users_reps = torch.zeros([all_user_embedding.shape[0], len(selected_submissions)])\n",
    "        for row_i, row in train_data.iterrows():\n",
    "            if row[\"USERNAME\"] in selected_users and row[\"SUBMISSION_ID\"] in selected_submissions:\n",
    "                vote = 1 if row[\"VOTE\"] == 1 else -1\n",
    "                users_reps[row[\"USERNAME\"], sub_sub_i_map[row[\"SUBMISSION_ID\"]]] = vote\n",
    "        selected_users_reps = users_reps[selected_users_bool_vec, :]\n",
    "        users_vote_sum = (selected_users_reps * selected_users_reps).sum(axis = -1, keepdim= True)\n",
    "        assert (users_vote_sum != 0).all()\n",
    "        selected_users_reps = selected_users_reps / users_vote_sum # average votes on each submission\n",
    "        debug(selected_users_reps = selected_users_reps)\n",
    "\n",
    "    return selected_users_reps, selected_user_i_user_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtain representations for active users\n",
    "\n",
    "User representation will be used to cluster users into groups if `user_grouping_method` is \"`neural`\" or \"`vote`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either use this one... (clustering using vote prediction score on submissions in this subreddit, make sure `pred_all_user_submission_vote_score_matrix` is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred_all_user_submission_vote_score_matrix is not None:\n",
    "    debug(pred_all_user_submission_vote_score_matrix=pred_all_user_submission_vote_score_matrix)\n",
    "    subreddit_submissions_bool_vec = get_bool_vec(subreddit_test_submissions[a_subreddit].keys(), pred_all_user_submission_vote_score_matrix.shape[1])\n",
    "    a_subreddit_active_users_reps, a_subreddit_active_user_i_user_map = get_user_reps(a_subreddit_active_users, all_user_embedding=pred_all_user_submission_vote_score_matrix[:, subreddit_submissions_bool_vec], train_data=train_data, selected_submissions = subreddit_train_submissions[a_subreddit], method = user_grouping_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or this one... (cluster using user_embedding or sparse actual votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:18:46 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['a_subreddit_active_users_reps'], at \u001b[0m\u001b[1;32m<ipython-input-35-7650cac7360e>:9 <module>\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 22.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36ma_subreddit_active_users_reps\u001b[0m\u001b[1;33m Tensor size: torch.Size([47, 256]) val: tensor([[ 0.0434, -0.0086,  0.0045,  ..., -0.0290, -0.0233,  0.0251],\n",
      "        [-0.0078,  0.0225, -0.0161,  ..., -0.0110,  0.0162,  0.0012],\n",
      "        [-0.0029, -0.0006,  0.0027,  ..., -0.0151, -0.0027, -0.0049],\n",
      "        ...,\n",
      "        [ 0.0268, -0.0098,  0.0025,  ...,  0.0135, -0.0003,  0.0060],\n",
      "        [ 0.0031,  0.0072,  0.0112,  ..., -0.0009, -0.0378,  0.0079],\n",
      "        [ 0.0192, -0.0035, -0.0071,  ...,  0.0204,  0.0253, -0.0075]],\n",
      "       device='cuda:3')\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:18:46 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if pred_all_user_submission_vote_score_matrix is None:\n",
    "    all_username_tokens = [f\"USERNAME_{user_i}\" for user_i in all_users]\n",
    "    all_username_token_ids = torch.tensor(model.tokenizer.convert_tokens_to_ids(all_username_tokens))\n",
    "    all_username_token_ids = all_username_token_ids.to(model.device); model = model.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        user_embedding = model.lm_encoder.embeddings.word_embeddings(all_username_token_ids)\n",
    "    # debug(all_username_tokens=all_username_tokens, all_username_token_ids=all_username_token_ids, user_embedding=user_embedding)\n",
    "    a_subreddit_active_users_reps, a_subreddit_active_user_i_user_map = get_user_reps(a_subreddit_active_users, all_user_embedding=user_embedding, train_data=train_data, selected_submissions = subreddit_train_submissions[a_subreddit], method = user_grouping_method)\n",
    "    debug(a_subreddit_active_users_reps=a_subreddit_active_users_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster active users into multiple groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:20:12 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 2 vars: ['num_selected_users', 'n_groups'], at \u001b[0m\u001b[1;32m<ipython-input-42-096fc941a2e3>:14 get_user_groups\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 35.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mnum_selected_users\u001b[0m\u001b[1;33m num val: 47\u001b[0m\n",
      "\u001b[1;33m1 / 36.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mn_groups\u001b[0m\u001b[1;33m num val: 9\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:20:12 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:20:12 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m<ipython-input-42-096fc941a2e3>:15 get_user_groups\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mBegin grouping...\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:20:12 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:20:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['group_user_num'], at \u001b[0m\u001b[1;32m<ipython-input-42-096fc941a2e3>:38 get_user_groups\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 37.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36mgroup_user_num\u001b[0m\u001b[1;33m str len 55: {5: 1, 3: 36, 0: 4, 2: 1, 7: 1, 6: 1, 8: 1, 1: 1, 4: 1}\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:20:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:20:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m 1 vars: ['usernames_in_groups'], at \u001b[0m\u001b[1;32m<ipython-input-42-096fc941a2e3>:39 get_user_groups\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m0 / 38.\u001b[0m \u001b[1;33m \u001b[0m\u001b[1;36musernames_in_groups\u001b[0m\u001b[1;33m str len 736: defaultdict(<class 'set'>, {5: {'Babi_Gurrl'}, 3: {'JoA_MoN', 'ChokingTermite', 'Cassieisnotclever', 'TheQuips', 'dilkoman', 'exhuma', 'Reading_Otter', 'rich1138', 'kalel1980', 'vinsite', 'hosieryadvocate', 'jezebel_jessi', 'dreamindelay', 'rob263', 'aaaaa', 'CallMeJase', 'claudesoph', 'relaxedguy', 'reggie991', 'Tekuzo', 'LinkBrokeMyPots', 'LifeIsProbablyMadeUp', 'Swamptor', 'australiano', 'MRteddybear29', 'milleribsen', 'TigerWylde', 'fattermcgee', 'blurbie', 'Simba7', 'LoGMunKy', 'Kahandran', 'lgtbyddrk', 'Johnl ... , 4: {'yoduh4077'}})\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:20:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def get_user_groups(selected_users_reps, selected_user_i_user_map:dict, user_grouping_method = \"neural\", existing_user_votes=None, manual_user_groups=None):\n",
    "    group_centers = None\n",
    "    if user_grouping_method == \"single_user_as_group\":\n",
    "        assert existing_user_votes is not None\n",
    "        users_in_groups = list(selected_user_i_user_map.values())\n",
    "        users_in_groups.sort(key=lambda x:existing_user_votes[x])\n",
    "        users_in_groups = users_in_groups[:10] + users_in_groups[-10:]\n",
    "        users_in_groups = {i: {user} for i,user in enumerate(users_in_groups)}\n",
    "        # users_in_groups = {i: {user} for i,user in selected_user_i_user_map.items()}\n",
    "    elif user_grouping_method == \"manual\":\n",
    "        users_in_groups = manual_user_groups\n",
    "    else:\n",
    "        n_groups = int(len(selected_user_i_user_map) / 5) # TODO: change how many users in a group\n",
    "        debug(num_selected_users = len(selected_user_i_user_map), n_groups=n_groups) # n_groups: 118\n",
    "        debug(\"Begin grouping...\")\n",
    "        from sklearn.cluster import KMeans\n",
    "        grouping = KMeans(n_clusters = n_groups, random_state = 42, verbose = 0).fit(selected_users_reps)\n",
    "        group_centers = grouping.cluster_centers_\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        grouping = AgglomerativeClustering(linkage = \"complete\").fit(selected_users_reps)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import SpectralClustering\n",
    "        grouping = SpectralClustering(n_groups, random_state = 42, verbose = 0).fit(selected_users_reps)\n",
    "        \"\"\"\n",
    "        labels = grouping.labels_ # grouping.labels_: [584 350 948 ... 813 938 152]\n",
    "        \"\"\"\n",
    "        from sklearn.mixture import GaussianMixture\n",
    "        labels = GaussianMixture(n_groups, random_state = 42, verbose = 0).fit_predict(selected_users_reps)\n",
    "        \"\"\"\n",
    "        users_in_groups = defaultdict(set)\n",
    "        usernames_in_groups = defaultdict(set)\n",
    "        for user_i, group_x in enumerate(labels): \n",
    "            users_in_groups[group_x].add(selected_user_i_user_map[user_i])\n",
    "            usernames_in_groups[group_x].add(original_feature_map[\"USERNAME\"][selected_user_i_user_map[user_i]])\n",
    "        assert len(join_sets(users_in_groups.values())) == sum([len(users) for users in users_in_groups.values()])\n",
    "        debug(group_user_num=str({group_x: len(users_in_groups[group_x]) for group_x in users_in_groups}))\n",
    "        debug(usernames_in_groups=str(usernames_in_groups))\n",
    "    return users_in_groups, group_centers\n",
    "a_subreddit_active_users_reps = a_subreddit_active_users_reps.cpu()\n",
    "users_in_groups, group_centers = get_user_groups(a_subreddit_active_users_reps, a_subreddit_active_user_i_user_map, user_grouping_method=user_grouping_method, existing_user_votes=existing_user_votes, manual_user_groups=manual_user_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict preferred submissions of each group\n",
    "\n",
    "And show their relationship using venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:20:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;32mDEBUG:\u001b[0m\u001b[1;33m at \u001b[0m\u001b[1;32m/home/TableSense/largedisk/wanrong/Curation-Modeling/utils.py:123 print_log\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;36mPredicting group 3 with users {7811, 27395, 14601, 17929, 7563, 22410, 15381, 27548, 24350, 8738, 17190, 8617, 19378, 8757, 15029, 25272, 26812, 27461, 30407, 20552, 2507, 14290, 7509, 23254, 8663, 2904, 20313, 21211, 9067, 18539, 21103, 2676, 23669, 27384, 12155, 13436}\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m------------------\u001b[0m\u001b[1;31m 2022-08-06 03:20:13 \u001b[0m\u001b[1;33m------------------\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:00<00:00, 2087.12it/s]\n",
      "100%|██████████| 85/85 [00:09<00:00,  9.08it/s]\n",
      " 69%|██████▉   | 14991/21744 [08:36<01:56, 57.86it/s]"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "submission_text_map = test_data[[\"SUBMISSION_ID\", \"SUBMISSION_TEXT\"]].drop_duplicates(\"SUBMISSION_ID\").set_index(\"SUBMISSION_ID\").to_dict()['SUBMISSION_TEXT']\n",
    "\n",
    "def predict_groups_preferences(users_in_groups, unique_submissions:dict, train_data, group_centers=None, user_grouping_method = \"rep\", existing_votes = None, existing_user_updown_votes=None):\n",
    "    # users_in_groups = existing_user_votes.most_common(3)\n",
    "    groups_preferred_submissions = {}\n",
    "    groups_preferred_submissions_text = {}\n",
    "    used_group_centers = []\n",
    "    if os.path.exists(config[\"preferred_submissions_venn_figure_dir\"]):\n",
    "        shutil.rmtree(config[\"preferred_submissions_venn_figure_dir\"])\n",
    "    os.makedirs(config[\"preferred_submissions_venn_figure_dir\"], exist_ok=True)\n",
    "    for group_x in users_in_groups:\n",
    "        if (user_grouping_method != \"single_user_as_group\") and (len(users_in_groups[group_x]) <= config[\"group_user_num_lower_thres\"] or len(users_in_groups[group_x]) > config[\"group_user_num_upper_thres\"]): # keep middle sized centers\n",
    "            continue\n",
    "        if group_centers is not None: # only keep not similar centers\n",
    "            group_x_center = group_centers[group_x]\n",
    "            similar_center = False\n",
    "            for center in used_group_centers:\n",
    "                if np.dot(group_x_center, center) > 0:\n",
    "                    similar_center = True\n",
    "                    break\n",
    "            if similar_center:\n",
    "                # continue\n",
    "                print_log(config[\"log_path\"], \"Have similar center with existing group\")\n",
    "            used_group_centers.append(group_x_center)\n",
    "\n",
    "        ################ predicting votes of some users and some submissions ##################\n",
    "        print_log(config[\"log_path\"], f\"Predicting group {group_x} with users {users_in_groups[group_x]}\")\n",
    "        \n",
    "        group_x_subreddit_submissions_data = convert_group_users_subreddit_submissions_data(users_in_groups[group_x], unique_submissions)\n",
    "        predicted_group_x_submissions_votes = predict_group_users_submissions_votes(model, group_x_subreddit_submissions_data, batch_size)\n",
    "        group_x_preferred_submissions, group_x_preferred_submissions_ranking, group_x_submission_votes, group_x_confidence, pred_group_x_subreddit_submission_vote_score_matrix, pred_group_x_subreddit_submission_vote_matrix = get_group_users_preferred_submissions(model, predicted_group_x_submissions_votes, users_in_groups[group_x], group_x_subreddit_submissions_data, train_data, existing_votes, thres = config[\"upvote_downvote_ratio_thres\"], existing_pred_submission_user_vote_score_matrix = pred_all_user_submission_vote_score_matrix) # can delete existing_pred_submission_user_vote_score_matrix if not available\n",
    "\n",
    "        ################# Display submissions preferred by each group of users ######################\n",
    "\n",
    "        if user_grouping_method == \"single_user_as_group\":\n",
    "            user_train_vote_prompt = f\"voted {existing_user_updown_votes[list(users_in_groups[group_x])[0]]} in training data, prediction confidence {list(group_x_confidence.values())[0]}, \"\n",
    "        else:\n",
    "            user_train_vote_prompt = \"\"\n",
    "            \n",
    "        group_x_preferred_ranked_submissions = group_x_preferred_submissions_ranking[:len(group_x_preferred_submissions)]\n",
    "        groups_preferred_submissions[f\"Group {group_x}\"] = group_x_preferred_submissions\n",
    "        \n",
    "        # convert submission text content\n",
    "        group_x_preferred_ranked_submissions_text = []\n",
    "        for submission_id in group_x_preferred_ranked_submissions:\n",
    "            if \"SUBMISSION_ID\" in original_feature_map:\n",
    "                submission_id = original_feature_map[\"SUBMISSION_ID\"][submission_id]\n",
    "            group_x_preferred_ranked_submissions_text.append(submission_text_map[submission_id])\n",
    "        groups_preferred_submissions_text[f\"Group {group_x}\"] = group_x_preferred_ranked_submissions_text\n",
    "            \n",
    "        print_log(config[\"log_path\"], f\"Users in group {group_x} {user_train_vote_prompt}prefers {len(group_x_preferred_submissions)}/{len(unique_submissions)} submissions (sorted using %upvotes): {group_x_preferred_ranked_submissions}, with text {group_x_preferred_ranked_submissions_text}\")\n",
    "\n",
    "        # draw venn diagram            \n",
    "        if len(groups_preferred_submissions) > 1 and len(groups_preferred_submissions) <=6:\n",
    "            ax = venn(groups_preferred_submissions) if len(groups_preferred_submissions) <=5 else pseudovenn(groups_preferred_submissions)\n",
    "            plt.show()\n",
    "            figure_path = f\"{config['preferred_submissions_venn_figure_dir']}/{len(groups_preferred_submissions)}_groups.png\"\n",
    "            ax.figure.savefig(figure_path)\n",
    "            debug(f\"Figure saved in {figure_path}\")\n",
    "    return groups_preferred_submissions, groups_preferred_submissions_text\n",
    "\n",
    "\n",
    "model = model.to(model.device); model.eval()\n",
    "groups_preferred_submissions, groups_preferred_submissions_text = predict_groups_preferences(users_in_groups, subreddit_test_submissions[a_subreddit], train_data, group_centers, user_grouping_method=user_grouping_method, existing_votes=existing_votes, existing_user_updown_votes=existing_user_updown_votes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12 ('cr4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "740471d2bc5e6e0b41d12bdc2e64373746aa6a34800f381ff958ff5f02fa0c53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
